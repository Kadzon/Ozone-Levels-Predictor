{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugmm77OH3U-o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict, Optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN, Conv1D, Flatten, MaxPooling1D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note\n",
        "\n",
        "This uses datasets used in the paper below:\n",
        "\n",
        "[Analysis and prediciton of atmospheric ozone concentrations using machine learning](https://www.researchgate.net/publication/388531615_Analysis_and_prediction_of_atmospheric_ozone_concentrations_using_machine_learning/fulltext/67b1263e645ef274a481f1b1/Analysis-and-prediction-of-atmospheric-ozone-concentrations-using-machine-learning.pdf)"
      ],
      "metadata": {
        "id": "4BwAaqa_RWWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Model"
      ],
      "metadata": {
        "id": "LVLmuJ3o6B8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extended version of the NABEL data loader that includes all parameters\n",
        "needed for the non-linear ozone prediction model (Model 4).\n",
        "\"\"\"\n",
        "\n",
        "class ExtendedNABELDataLoader:\n",
        "    def __init__(self, data_dir: str = \"/content/drive/MyDrive/MBD-FRP/data/raw\"):\n",
        "        \"\"\"\n",
        "        Initialize the extended NABEL data loader.\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Directory where raw NABEL data files are stored\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.station_name = \"Lugano-Università\"\n",
        "        self.required_params = ['O3', 'NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "        self.all_params = self.required_params + ['CO', 'PM10', 'PM2.5', 'EC', 'CPC', 'PREC']\n",
        "\n",
        "    def load_parameter_file(self, param_name: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load data for a specific parameter from CSV file.\n",
        "\n",
        "        Args:\n",
        "            param_name (str): Name of parameter to load\n",
        "\n",
        "        Returns:\n",
        "            Optional[pd.DataFrame]: DataFrame with date and parameter value, or None if file not found\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(self.data_dir, f\"{param_name}.csv\")\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: File not found for parameter {param_name}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Load file, skip header rows and rename columns\n",
        "            df = pd.read_csv(file_path, sep=';', skiprows=4, encoding='latin1')\n",
        "\n",
        "            # Extract only first two columns (date and value)\n",
        "            df = df.iloc[:, :2]\n",
        "\n",
        "            # Rename columns\n",
        "            df.columns = ['date', param_name.lower()]\n",
        "\n",
        "            # Convert date to datetime\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')\n",
        "\n",
        "            # Convert values to numeric, handling any non-numeric values\n",
        "            df[param_name.lower()] = pd.to_numeric(df[param_name.lower()].astype(str).str.replace(',', '.'),\n",
        "                                                errors='coerce')\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {param_name} data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_and_prepare_data(self, start_year: int = 2016,\n",
        "                            end_year: int = 2023) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and prepare NABEL data for the specified time period.\n",
        "\n",
        "        Args:\n",
        "            start_year (int): Start year for data loading\n",
        "            end_year (int): End year for data loading\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Prepared dataset with daily averages\n",
        "        \"\"\"\n",
        "        # Load data for each parameter\n",
        "        data_frames = {}\n",
        "        for param in self.all_params:\n",
        "            df = self.load_parameter_file(param)\n",
        "            if df is not None:\n",
        "                data_frames[param] = df\n",
        "\n",
        "        # Check if all required parameters are loaded\n",
        "        missing_required = [param for param in self.required_params if param not in data_frames]\n",
        "        if missing_required:\n",
        "            raise ValueError(f\"Missing required parameters: {missing_required}\")\n",
        "\n",
        "        # Start with ozone data\n",
        "        merged_df = data_frames['O3']\n",
        "\n",
        "        # Merge with other parameters\n",
        "        for param, df in data_frames.items():\n",
        "            if param != 'O3':\n",
        "                merged_df = pd.merge(merged_df, df, on='date', how='inner')\n",
        "\n",
        "        # Filter by date range\n",
        "        mask = (merged_df['date'].dt.year >= start_year) & (merged_df['date'].dt.year <= end_year)\n",
        "        merged_df = merged_df[mask].copy()\n",
        "\n",
        "        # Sort by date\n",
        "        merged_df = merged_df.sort_values('date')\n",
        "\n",
        "        # Rename columns to standard format expected by models\n",
        "        column_mapping = {\n",
        "            'o3': 'ozone',\n",
        "            'rad': 'RAD',\n",
        "            'no2': 'NO2',\n",
        "            'nox': 'NOX',\n",
        "            'so2': 'SO2',\n",
        "            'nmvoc': 'NMVOC',\n",
        "            'temp': 'TEMP',\n",
        "            'co': 'CO',\n",
        "            'pm10': 'PM10',\n",
        "            'pm2.5': 'PM2.5',\n",
        "            'ec': 'EC',\n",
        "            'cpc': 'CPC',\n",
        "            'prec': 'PREC'\n",
        "        }\n",
        "\n",
        "        merged_df = merged_df.rename(columns={k: v for k, v in column_mapping.items()\n",
        "                                            if k in merged_df.columns})\n",
        "\n",
        "        return merged_df\n",
        "\n",
        "    def process_raw_data(self, df: pd.DataFrame, handle_missing: str = 'drop') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Process raw NABEL data.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Raw data from NABEL\n",
        "            handle_missing (str): Strategy for handling missing values ('drop' or 'interpolate')\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Processed data\n",
        "        \"\"\"\n",
        "        # Handle missing values\n",
        "        if handle_missing == 'drop':\n",
        "            # Simply drop rows with any missing values\n",
        "            df = df.dropna()\n",
        "        elif handle_missing == 'interpolate':\n",
        "            # Interpolate missing values\n",
        "            df = df.set_index('date')\n",
        "            df = df.interpolate(method='time')\n",
        "            df = df.reset_index()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid handle_missing strategy: {handle_missing}\")\n",
        "\n",
        "        # Add derived columns that might be useful\n",
        "        df['Year'] = df['date'].dt.year\n",
        "        df['Month'] = df['date'].dt.month\n",
        "        df['DayOfYear'] = df['date'].dt.dayofyear\n",
        "\n",
        "        # Add seasonality features (based on day of year)\n",
        "        df['Season_Sin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365)\n",
        "        df['Season_Cos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def split_data(self, df: pd.DataFrame,\n",
        "                  test_size: float = 0.2,\n",
        "                  random_state: int = 42,\n",
        "                  time_based: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Split data into training and testing sets.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Processed daily data\n",
        "            test_size (float): Proportion of data to use for testing\n",
        "            random_state (int): Random seed for reproducibility\n",
        "            time_based (bool): Whether to split based on time (True) or randomly (False)\n",
        "\n",
        "        Returns:\n",
        "            Tuple[pd.DataFrame, pd.DataFrame]: Training and testing datasets\n",
        "        \"\"\"\n",
        "        if time_based:\n",
        "            # Split based on time (maintain temporal order)\n",
        "            split_idx = int(len(df) * (1 - test_size))\n",
        "            train_data = df.iloc[:split_idx].copy()\n",
        "            test_data = df.iloc[split_idx:].copy()\n",
        "        else:\n",
        "            # Random split (not maintaining temporal order)\n",
        "            train_data, test_data = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        return train_data, test_data\n",
        "\n",
        "    def validate_features(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Validate that the dataframe contains all required features for the non-linear model.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame to validate\n",
        "\n",
        "        Returns:\n",
        "            bool: True if all required features are present\n",
        "        \"\"\"\n",
        "        required_cols = ['ozone', 'NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "            print(f\"Missing required columns: {missing_cols}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "def dataloader():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate usage of the ExtendedNABELDataLoader class.\n",
        "    \"\"\"\n",
        "    loader = ExtendedNABELDataLoader()\n",
        "\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        print(\"Loading and preparing data...\")\n",
        "        data = loader.load_and_prepare_data()\n",
        "\n",
        "        # Process data\n",
        "        print(\"Processing data...\")\n",
        "        processed_data = loader.process_raw_data(data, handle_missing='interpolate')\n",
        "\n",
        "        # Validate that we have all required features\n",
        "        if not loader.validate_features(processed_data):\n",
        "            raise ValueError(\"Dataset is missing required features for the non-linear model\")\n",
        "\n",
        "        # Split data\n",
        "        print(\"Splitting data into train and test sets...\")\n",
        "        train_data, test_data = loader.split_data(processed_data, test_size=0.2, time_based=True)\n",
        "\n",
        "        # Save processed data\n",
        "        print(\"Saving processed data...\")\n",
        "        processed_dir = \"data/processed\"\n",
        "        os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "        train_data.to_csv(os.path.join(processed_dir, \"train_data_full.csv\"), index=False)\n",
        "        test_data.to_csv(os.path.join(processed_dir, \"test_data_full.csv\"), index=False)\n",
        "\n",
        "        print(\"\\nData processing completed successfully!\")\n",
        "        print(f\"Total samples: {len(processed_data)}\")\n",
        "        print(f\"Training samples: {len(train_data)}\")\n",
        "        print(f\"Testing samples: {len(test_data)}\")\n",
        "\n",
        "        # Print data summary\n",
        "        print(\"\\nData Summary:\")\n",
        "        print(processed_data.describe())\n",
        "\n",
        "        # Print feature correlations with ozone\n",
        "        print(\"\\nFeature Correlations with Ozone:\")\n",
        "        correlations = processed_data.corr()['ozone'].sort_values(ascending=False)\n",
        "        print(correlations)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpU1e1Tf8P-w",
        "outputId": "3048837e-c44a-46c3-8fe3-53422cf69bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Warning: File not found for parameter CO\n",
            "Warning: File not found for parameter PM10\n",
            "Warning: File not found for parameter PM2.5\n",
            "Warning: File not found for parameter EC\n",
            "Warning: File not found for parameter CPC\n",
            "Warning: File not found for parameter PREC\n",
            "Processing data...\n",
            "Splitting data into train and test sets...\n",
            "Saving processed data...\n",
            "\n",
            "Data processing completed successfully!\n",
            "Total samples: 2922\n",
            "Training samples: 2337\n",
            "Testing samples: 585\n",
            "\n",
            "Data Summary:\n",
            "                      date        ozone          NO2          NOX  \\\n",
            "count                 2922  2922.000000  2922.000000  2922.000000   \n",
            "mean   2019-12-31 12:00:00    91.406742    23.685832    33.555065   \n",
            "min    2016-01-01 00:00:00     3.600000     3.800000     4.100000   \n",
            "25%    2017-12-31 06:00:00    60.325000    13.200000    14.900000   \n",
            "50%    2019-12-31 12:00:00    88.700000    20.400000    24.300000   \n",
            "75%    2021-12-30 18:00:00   122.175000    32.300000    44.400000   \n",
            "max    2023-12-31 00:00:00   199.400000    79.800000   228.700000   \n",
            "std                    NaN    42.961556    12.980111    26.133903   \n",
            "\n",
            "               SO2        NMVOC         TEMP          RAD         Year  \\\n",
            "count  2922.000000  2922.000000  2922.000000  2922.000000  2922.000000   \n",
            "mean      1.442060     0.105082    13.763039   163.431759  2019.498973   \n",
            "min       0.000000     0.000000    -2.800000     1.900000  2016.000000   \n",
            "25%       0.500000     0.100000     7.600000    76.600000  2017.250000   \n",
            "50%       0.900000     0.100000    13.400000   150.550000  2019.500000   \n",
            "75%       2.100000     0.100000    20.200000   250.725000  2021.000000   \n",
            "max       6.500000     0.400000    28.800000   363.400000  2023.000000   \n",
            "std       1.275779     0.054781     7.107021   100.902581     2.291829   \n",
            "\n",
            "             Month    DayOfYear    Season_Sin   Season_Cos  \n",
            "count  2922.000000  2922.000000  2.922000e+03  2922.000000  \n",
            "mean      6.522930   183.125257  1.178190e-05     0.000684  \n",
            "min       1.000000     1.000000 -9.999907e-01    -0.999963  \n",
            "25%       4.000000    92.000000 -7.055836e-01    -0.708627  \n",
            "50%       7.000000   183.000000  6.432491e-16     0.004304  \n",
            "75%      10.000000   274.000000  7.055836e-01     0.702527  \n",
            "max      12.000000   366.000000  9.999907e-01     1.000000  \n",
            "std       3.449293   105.456690  7.069859e-01     0.707469  \n",
            "\n",
            "Feature Correlations with Ozone:\n",
            "ozone         1.000000\n",
            "RAD           0.832332\n",
            "TEMP          0.805178\n",
            "Season_Sin    0.046087\n",
            "Year          0.016082\n",
            "date          0.003617\n",
            "Month        -0.097719\n",
            "DayOfYear    -0.098824\n",
            "NMVOC        -0.345359\n",
            "SO2          -0.562379\n",
            "NO2          -0.690206\n",
            "NOX          -0.692046\n",
            "Season_Cos   -0.809629\n",
            "Name: ozone, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OzoneBaselineModel:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the baseline ozone prediction model.\"\"\"\n",
        "        self.model = LinearRegression()\n",
        "        self.metrics = {}\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Train the linear regression model.\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features (radiation)\n",
        "            y_train (np.ndarray): Training targets (ozone concentration)\n",
        "        \"\"\"\n",
        "        self.model.fit(X_train.reshape(-1, 1), y_train)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features (radiation)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted ozone concentrations\n",
        "        \"\"\"\n",
        "        return self.model.predict(X.reshape(-1, 1))\n",
        "\n",
        "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (np.ndarray): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        self.metrics = {\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'r2': self.model.score(X_test.reshape(-1, 1), y_test),\n",
        "            'coefficient': float(self.model.coef_[0]),\n",
        "            'intercept': float(self.model.intercept_)\n",
        "        }\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_results(self, X_test: np.ndarray, y_test: np.ndarray,\n",
        "                    dates_test: pd.DatetimeIndex = None,\n",
        "                    save_dir: str = \"results/figures\") -> None:\n",
        "        \"\"\"\n",
        "        Create and save various plots to analyze model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (np.ndarray): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "            dates_test (pd.DatetimeIndex): Test dates for time series plot\n",
        "            save_dir (str): Directory to save the plots\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Create figures directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Scatter plot with regression line\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(X_test, y_test, alpha=0.5, label='Actual')\n",
        "        plt.plot(X_test, y_pred, color='red', label='Predicted')\n",
        "        plt.xlabel('Radiation (W/m²)')\n",
        "        plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "        plt.title('Baseline Model: Ozone Concentration vs Radiation')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"scatter_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Residual plot\n",
        "        residuals = y_test - y_pred\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Residuals (µg/m³)')\n",
        "        plt.title('Residual Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Histogram of residuals\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.xlabel('Residuals (µg/m³)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Residuals')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Time series plot (if dates are provided)\n",
        "        if dates_test is not None:\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_test, y_test, label='Actual', alpha=0.7)\n",
        "            plt.plot(dates_test, y_pred, label='Predicted', alpha=0.7)\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "            plt.title('Ozone Concentration Over Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "    def save_metrics(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the metrics\n",
        "        \"\"\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "def lin_baseline():\n",
        "    \"\"\"Main function to train and evaluate the baseline model.\"\"\"\n",
        "    # Create results directory\n",
        "    results_dir = \"results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(results_dir, \"figures\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(results_dir, \"models\"), exist_ok=True)\n",
        "\n",
        "    # Load processed data\n",
        "    processed_dir = \"data/processed\"\n",
        "    train_data = pd.read_csv(os.path.join(processed_dir, \"train_data.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(processed_dir, \"test_data.csv\"))\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # Extract features and targets\n",
        "    X_train = train_data['radiation'].values\n",
        "    y_train = train_data['ozone'].values\n",
        "    X_test = test_data['radiation'].values\n",
        "    y_test = test_data['ozone'].values\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = OzoneBaselineModel()\n",
        "    model.train(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics = model.evaluate(X_test, y_test)\n",
        "    print(\"\\nModel Evaluation Metrics:\")\n",
        "    print(f\"MAE: {metrics['mae']:.2f} µg m⁻³\")\n",
        "    print(f\"RMSE: {metrics['rmse']:.2f} µg m⁻³\")\n",
        "    print(f\"R²: {metrics['r2']:.3f}\")\n",
        "    print(f\"\\nModel Equation:\")\n",
        "    print(f\"[O₃] = {metrics['coefficient']:.3f} · Radiation + {metrics['intercept']:.3f} µg m⁻³\")\n",
        "\n",
        "    # Create visualizations\n",
        "    model.plot_results(X_test, y_test, test_data['date'])\n",
        "\n",
        "    # Save metrics\n",
        "    model.save_metrics(os.path.join(results_dir, \"models\", \"baseline_metrics.json\"))\n",
        "\n",
        "\n",
        "\n",
        "lin_baseline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufPoWaZl5SpU",
        "outputId": "fd8355e0-c066-4470-c790-5a95dac1d36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Metrics:\n",
            "MAE: 17.15 µg m⁻³\n",
            "RMSE: 21.57 µg m⁻³\n",
            "R²: 0.735\n",
            "\n",
            "Model Equation:\n",
            "[O₃] = 0.350 · Radiation + 33.962 µg m⁻³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear Model"
      ],
      "metadata": {
        "id": "QrmnGXF26UHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Non-linear regression model for ozone concentration prediction.\n",
        "Based on Model 4 from the research paper using 12 key components of polynomial features.\n",
        "\"\"\"\n",
        "\n",
        "class OzoneNonLinearModel:\n",
        "    def __init__(self, n_components: int = 12, degree: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the non-linear ozone prediction model.\n",
        "\n",
        "        Args:\n",
        "            n_components (int): Number of components (terms) to include in the model\n",
        "            degree (int): Degree of polynomial features to generate\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.degree = degree\n",
        "        self.base_features = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "\n",
        "        # Create polynomial features transformer (no column names to avoid warnings)\n",
        "        self.poly = PolynomialFeatures(degree=self.degree, include_bias=True)\n",
        "\n",
        "        # Create linear regression model\n",
        "        self.model = LinearRegression()\n",
        "\n",
        "        # Store metrics and important components\n",
        "        self.metrics = {}\n",
        "        self.selected_features = []\n",
        "        self.feature_importances = {}\n",
        "        self.selected_indices = None  # Will store indices of selected features\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Train the non-linear regression model.\n",
        "\n",
        "        Args:\n",
        "            X_train (pd.DataFrame): Training features (must include the 6 base features)\n",
        "            y_train (np.ndarray): Training targets (ozone concentration)\n",
        "        \"\"\"\n",
        "        # Check if all required features are present\n",
        "        missing_features = [feat for feat in self.base_features if feat not in X_train.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "\n",
        "        # Extract features in the correct order\n",
        "        X = X_train[self.base_features].values\n",
        "\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.poly.fit_transform(X)\n",
        "\n",
        "        # Get feature names\n",
        "        self.feature_names = self.poly.get_feature_names_out(self.base_features)\n",
        "\n",
        "        # Train full model first to determine important components\n",
        "        self.full_model = LinearRegression().fit(X_poly, y_train)\n",
        "\n",
        "        # Select the most important components\n",
        "        self._select_important_components()\n",
        "\n",
        "        # Use only selected components for final model\n",
        "        selected_X_poly = self._get_selected_features(X_poly)\n",
        "\n",
        "        # Train final model on selected components\n",
        "        self.model.fit(selected_X_poly, y_train)\n",
        "\n",
        "    def _select_important_components(self) -> None:\n",
        "        \"\"\"\n",
        "        Select the most important components based on coefficient magnitude.\n",
        "        \"\"\"\n",
        "        # Get absolute coefficients\n",
        "        coefs = self.full_model.coef_\n",
        "        abs_coefs = np.abs(coefs)\n",
        "\n",
        "        # Find indices of the most important components (excluding intercept/bias)\n",
        "        self.selected_indices = np.argsort(abs_coefs)[-self.n_components:]\n",
        "\n",
        "        # Store selected feature names\n",
        "        self.selected_features = [self.feature_names[i] for i in self.selected_indices]\n",
        "\n",
        "        # Store feature importances with their names\n",
        "        self.feature_importances = {\n",
        "            self.feature_names[i]: coefs[i] for i in self.selected_indices\n",
        "        }\n",
        "\n",
        "    def _get_selected_features(self, X_poly: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract only the selected features from polynomial features.\n",
        "\n",
        "        Args:\n",
        "            X_poly (np.ndarray): Full polynomial features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Selected polynomial features\n",
        "        \"\"\"\n",
        "        if self.selected_indices is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        # Extract only columns corresponding to selected indices\n",
        "        return X_poly[:, self.selected_indices]\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted ozone concentrations\n",
        "        \"\"\"\n",
        "        # Check if model is trained\n",
        "        if not hasattr(self, 'feature_names'):\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        # Check if all required features are present\n",
        "        missing_features = [feat for feat in self.base_features if feat not in X.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "\n",
        "        # Extract features in the correct order\n",
        "        X_feat = X[self.base_features].values\n",
        "\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.poly.transform(X_feat)\n",
        "\n",
        "        # Extract only selected features\n",
        "        selected_X_poly = self._get_selected_features(X_poly)\n",
        "\n",
        "        # Make predictions\n",
        "        return self.model.predict(selected_X_poly)\n",
        "\n",
        "    def evaluate(self, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Extract features for R² calculation\n",
        "        X_feat = X_test[self.base_features].values\n",
        "        X_poly = self.poly.transform(X_feat)\n",
        "        selected_X_poly = self._get_selected_features(X_poly)\n",
        "\n",
        "        self.metrics = {\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'r2': self.model.score(selected_X_poly, y_test),\n",
        "            'avg_predicted': float(np.mean(y_pred)),\n",
        "            'avg_actual': float(np.mean(y_test)),\n",
        "            'n_components': self.n_components\n",
        "        }\n",
        "\n",
        "        # Add coefficients to metrics\n",
        "        self.metrics['intercept'] = float(self.model.intercept_)\n",
        "        for i, idx in enumerate(self.selected_indices):\n",
        "            feature_name = self.feature_names[idx]\n",
        "            coef = self.model.coef_[i]\n",
        "            self.metrics[f'component_{i+1}_name'] = feature_name\n",
        "            self.metrics[f'component_{i+1}_coef'] = float(coef)\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def get_equation_string(self) -> str:\n",
        "        \"\"\"\n",
        "        Get the prediction equation as a formatted string.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted equation string\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'feature_names') or self.selected_indices is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        equation = \"[O₃] = \"\n",
        "\n",
        "        # Add intercept\n",
        "        equation += f\"{self.model.intercept_:.1f}\"\n",
        "\n",
        "        # Add each component\n",
        "        for i, idx in enumerate(self.selected_indices):\n",
        "            feature = self.feature_names[idx]\n",
        "            coef = self.model.coef_[i]\n",
        "\n",
        "            # Format coefficient (handle very small or very large values)\n",
        "            if abs(coef) < 0.001:\n",
        "                coef_str = f\"{coef:.1e}\"\n",
        "            elif abs(coef) < 0.01:\n",
        "                coef_str = f\"{coef:.4f}\"\n",
        "            elif abs(coef) < 0.1:\n",
        "                coef_str = f\"{coef:.3f}\"\n",
        "            elif abs(coef) < 1:\n",
        "                coef_str = f\"{coef:.2f}\"\n",
        "            else:\n",
        "                coef_str = f\"{coef:.1f}\"\n",
        "\n",
        "            # Add term with correct sign\n",
        "            if coef >= 0:\n",
        "                equation += f\" + {coef_str} · {feature}\"\n",
        "            else:\n",
        "                equation += f\" - {abs(coef):.1f} · {feature}\"\n",
        "\n",
        "        # Add units\n",
        "        equation += \" μg m⁻³\"\n",
        "\n",
        "        return equation\n",
        "\n",
        "    def plot_results(self, X_test: pd.DataFrame, y_test: np.ndarray,\n",
        "                    dates_test: pd.DatetimeIndex = None,\n",
        "                    save_dir: str = \"results/figures/nonlinear\") -> None:\n",
        "        \"\"\"\n",
        "        Create and save various plots to analyze model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "            dates_test (pd.DatetimeIndex): Test dates for time series plot\n",
        "            save_dir (str): Directory to save the plots\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Create figures directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Actual vs Predicted scatter plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "\n",
        "        # Add perfect prediction line\n",
        "        max_val = max(np.max(y_test), np.max(y_pred))\n",
        "        min_val = min(np.min(y_test), np.min(y_pred))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "        plt.xlabel('Actual Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.title('Non-linear Model: Actual vs Predicted Ozone Concentration')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add metrics text\n",
        "        r2 = self.metrics.get('r2', 0)\n",
        "        rmse = self.metrics.get('rmse', 0)\n",
        "        mae = self.metrics.get('mae', 0)\n",
        "\n",
        "        metrics_text = f\"R² = {r2:.3f}\\nRMSE = {rmse:.2f} µg/m³\\nMAE = {mae:.2f} µg/m³\"\n",
        "        plt.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"actual_vs_predicted.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Residual plot\n",
        "        residuals = y_test - y_pred\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Residuals (µg/m³)')\n",
        "        plt.title('Residual Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add residual statistics\n",
        "        res_mean = np.mean(residuals)\n",
        "        res_std = np.std(residuals)\n",
        "        res_text = f\"Mean = {res_mean:.2f} µg/m³\\nStd Dev = {res_std:.2f} µg/m³\"\n",
        "        plt.annotate(res_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Histogram of residuals\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.xlabel('Residuals (µg/m³)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Residuals')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Time series plot (if dates are provided)\n",
        "        if dates_test is not None:\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_test, y_test, label='Actual', alpha=0.7)\n",
        "            plt.plot(dates_test, y_pred, label='Predicted', alpha=0.7)\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "            plt.title('Ozone Concentration Over Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 5. Feature importance plot\n",
        "        if self.feature_importances:\n",
        "            # Convert to DataFrame for plotting\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': list(self.feature_importances.keys()),\n",
        "                'Coefficient': list(self.feature_importances.values())\n",
        "            })\n",
        "\n",
        "            # Sort by absolute coefficient value\n",
        "            importance_df['Abs_Coefficient'] = importance_df['Coefficient'].abs()\n",
        "            importance_df = importance_df.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Set colors based on coefficient sign (positive: blue, negative: red)\n",
        "            colors = ['royalblue' if c > 0 else 'crimson' for c in importance_df['Coefficient']]\n",
        "\n",
        "            sns.barplot(x='Coefficient', y='Feature', data=importance_df, palette=colors)\n",
        "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.title('Feature Importance in Non-linear Model')\n",
        "            plt.xlabel('Coefficient Value')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"feature_importance.png\"))\n",
        "            plt.close()\n",
        "\n",
        "    def save_metrics(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the metrics\n",
        "        \"\"\"\n",
        "        # Add equation string to metrics\n",
        "        self.metrics['equation'] = self.get_equation_string()\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "    def save_model(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model details (not the full model) to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the model details\n",
        "        \"\"\"\n",
        "        model_data = {\n",
        "            'n_components': self.n_components,\n",
        "            'degree': self.degree,\n",
        "            'base_features': self.base_features,\n",
        "            'selected_features': self.selected_features,\n",
        "            'selected_indices': self.selected_indices.tolist(),\n",
        "            'intercept': float(self.model.intercept_),\n",
        "            'coefficients': [float(coef) for coef in self.model.coef_],\n",
        "            'equation': self.get_equation_string(),\n",
        "            'metrics': self.metrics\n",
        "        }\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(model_data, f, indent=4)\n",
        "\n",
        "def nonlinear_model():\n",
        "    \"\"\"Main function to train and evaluate the non-linear model.\"\"\"\n",
        "    # Create results directory\n",
        "    results_dir = \"results\"\n",
        "    model_dir = os.path.join(results_dir, \"models\")\n",
        "    figures_dir = os.path.join(results_dir, \"figures/nonlinear\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # Load processed data with all required features\n",
        "    processed_dir = \"data/processed\"\n",
        "    train_data = pd.read_csv(os.path.join(processed_dir, \"train_data_full.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(processed_dir, \"test_data_full.csv\"))\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # Extract features and targets\n",
        "    feature_cols = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "    X_train = train_data[feature_cols]\n",
        "    y_train = train_data['ozone'].values\n",
        "    X_test = test_data[feature_cols]\n",
        "    y_test = test_data['ozone'].values\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = OzoneNonLinearModel(n_components=12, degree=3)\n",
        "    model.train(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics = model.evaluate(X_test, y_test)\n",
        "    print(\"\\nNon-linear Model Evaluation Metrics:\")\n",
        "    print(f\"MAE: {metrics['mae']:.2f} µg m⁻³\")\n",
        "    print(f\"RMSE: {metrics['rmse']:.2f} µg m⁻³\")\n",
        "    print(f\"R²: {metrics['r2']:.3f}\")\n",
        "\n",
        "    print(\"\\nModel Equation:\")\n",
        "    print(model.get_equation_string())\n",
        "\n",
        "    # Print most important components\n",
        "    print(\"\\nMost Important Components:\")\n",
        "    for feature, coef in sorted(model.feature_importances.items(),\n",
        "                              key=lambda x: abs(x[1]), reverse=True)[:12]:\n",
        "        print(f\"{feature}: {coef:.6f}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    model.plot_results(X_test, y_test, test_data['date'])\n",
        "\n",
        "    # Save metrics and model\n",
        "    model.save_metrics(os.path.join(model_dir, \"nonlinear_metrics.json\"))\n",
        "    model.save_model(os.path.join(model_dir, \"nonlinear_model.json\"))\n",
        "\n",
        "\n",
        "nonlinear_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-P_p5GV6aNV",
        "outputId": "c52f9d3e-3d0c-4c93-d407-213782f573cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Non-linear Model Evaluation Metrics:\n",
            "MAE: 20.29 µg m⁻³\n",
            "RMSE: 24.86 µg m⁻³\n",
            "R²: 0.654\n",
            "\n",
            "Model Equation:\n",
            "[O₃] = 38.7 + 520.0 · SO2 NMVOC^2 - 11.7 · NOX NMVOC^2 + 3.4 · TEMP - 17.9 · SO2^2 NMVOC - 205.3 · NMVOC^2 TEMP + 44.0 · NMVOC TEMP - 5.5 · NO2 NMVOC^2 + 11.7 · SO2 - 14.6 · SO2 NMVOC + 962.5 · NMVOC^2 - 478.1 · NMVOC + 4094.6 · NMVOC^3 μg m⁻³\n",
            "\n",
            "Most Important Components:\n",
            "NMVOC^3: -1190.101631\n",
            "NMVOC: 291.995193\n",
            "NMVOC^2: -215.912660\n",
            "SO2 NMVOC: -119.270920\n",
            "SO2: -41.575159\n",
            "NO2 NMVOC^2: 37.052094\n",
            "NMVOC TEMP: -25.458000\n",
            "NMVOC^2 TEMP: 23.750433\n",
            "SO2^2 NMVOC: 18.168111\n",
            "TEMP: -13.674769\n",
            "NOX NMVOC^2: -10.935469\n",
            "SO2 NMVOC^2: -10.553752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4edf3391b82e>:319: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x='Coefficient', y='Feature', data=importance_df, palette=colors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Non-linear regression model for ozone concentration prediction.\n",
        "Based on Model 4 from the research paper, using the exact 12 components specified in the paper.\n",
        "\"\"\"\n",
        "\n",
        "class OzoneNonLinearModelExact:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the non-linear ozone prediction model with the exact 12 components\n",
        "        from the paper by Räss and Leuenberger.\n",
        "        \"\"\"\n",
        "        self.base_features = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "\n",
        "        # Create polynomial features transformer with degree 3\n",
        "        self.poly = PolynomialFeatures(degree=3, include_bias=True)\n",
        "\n",
        "        # Create linear regression model\n",
        "        self.model = LinearRegression()\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics = {}\n",
        "\n",
        "        # The 12 components from the paper (will be set during training)\n",
        "        self.component_indices = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def _find_component_indices(self):\n",
        "        \"\"\"\n",
        "        Find the indices of the 12 specific components mentioned in the paper:\n",
        "\n",
        "        1. NO2³\n",
        "        2. NOX (linear term)\n",
        "        3. NOX²\n",
        "        4. NOX³\n",
        "        5. T³ (temperature cubed)\n",
        "        6. NO2 · T (interaction)\n",
        "        7. NO2 · I (interaction with radiation)\n",
        "        8. SO2 · NMVOC (interaction)\n",
        "        9. SO2 · T (interaction)\n",
        "        10. NMVOC · I (interaction)\n",
        "        11. NOX · T (interaction)\n",
        "        12. NOX · I (interaction)\n",
        "        \"\"\"\n",
        "        # Define the 12 components we want to use (based on paper description)\n",
        "        desired_components = [\n",
        "            'NO2^3',            # NO2 cubed\n",
        "            'NOX',              # NOX linear\n",
        "            'NOX^2',            # NOX squared\n",
        "            'NOX^3',            # NOX cubed\n",
        "            'TEMP^3',           # Temperature cubed\n",
        "            'NO2 TEMP',         # NO2 · T interaction\n",
        "            'NO2 RAD',          # NO2 · I interaction\n",
        "            'SO2 NMVOC',        # SO2 · NMVOC interaction\n",
        "            'SO2 TEMP',         # SO2 · T interaction\n",
        "            'NMVOC RAD',        # NMVOC · I interaction\n",
        "            'NOX TEMP',         # NOX · T interaction\n",
        "            'NOX RAD',          # NOX · I interaction\n",
        "        ]\n",
        "\n",
        "        # Maps for different possible naming formats\n",
        "        component_formats = {\n",
        "            'NO2^3': ['NO2^3', 'NO2 NO2 NO2', 'NO2**3', 'NO2³'],\n",
        "            'NOX': ['NOX', 'NOX^1', 'NOX**1'],\n",
        "            'NOX^2': ['NOX^2', 'NOX NOX', 'NOX**2', 'NOX²'],\n",
        "            'NOX^3': ['NOX^3', 'NOX NOX NOX', 'NOX**3', 'NOX³'],\n",
        "            'TEMP^3': ['TEMP^3', 'TEMP TEMP TEMP', 'TEMP**3', 'TEMP³'],\n",
        "            'NO2 TEMP': ['NO2 TEMP', 'TEMP NO2'],\n",
        "            'NO2 RAD': ['NO2 RAD', 'RAD NO2'],\n",
        "            'SO2 NMVOC': ['SO2 NMVOC', 'NMVOC SO2'],\n",
        "            'SO2 TEMP': ['SO2 TEMP', 'TEMP SO2'],\n",
        "            'NMVOC RAD': ['NMVOC RAD', 'RAD NMVOC'],\n",
        "            'NOX TEMP': ['NOX TEMP', 'TEMP NOX'],\n",
        "            'NOX RAD': ['NOX RAD', 'RAD NOX']\n",
        "        }\n",
        "\n",
        "        # Find the indices of the desired components\n",
        "        indices = []\n",
        "        component_names = []\n",
        "\n",
        "        # Create a lookup table for faster matching\n",
        "        feature_dict = {name: idx for idx, name in enumerate(self.feature_names)}\n",
        "\n",
        "        # First handle the bias/intercept term\n",
        "        indices.append(0)  # Bias term is always at index 0\n",
        "        component_names.append('1')  # Bias term is represented by 1\n",
        "\n",
        "        # Find each of our desired components\n",
        "        for component in desired_components:\n",
        "            found = False\n",
        "            # Try all possible naming formats\n",
        "            for possible_name in component_formats[component]:\n",
        "                # Check for exact match\n",
        "                if possible_name in feature_dict:\n",
        "                    indices.append(feature_dict[possible_name])\n",
        "                    component_names.append(possible_name)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "                # If not found, try more flexible matching for polynomial features\n",
        "                if not found:\n",
        "                    for feat_name in self.feature_names:\n",
        "                        # For polynomial features, try matching individual terms\n",
        "                        terms = possible_name.split()\n",
        "                        if len(terms) > 1:\n",
        "                            # Check if all terms appear in the feature name\n",
        "                            if all(term in feat_name for term in terms):\n",
        "                                indices.append(feature_dict[feat_name])\n",
        "                                component_names.append(feat_name)\n",
        "                                found = True\n",
        "                                break\n",
        "\n",
        "            if not found:\n",
        "                # If we can't find the component, print a warning and continue\n",
        "                print(f\"Warning: Could not find component {component} in feature names\")\n",
        "                # As a fallback, let's use the most similar feature name\n",
        "                best_match = None\n",
        "                max_score = 0\n",
        "                for name in self.feature_names:\n",
        "                    # Simple similarity score: count matching words\n",
        "                    score = sum(1 for term in component.split() if term in name.split())\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        best_match = name\n",
        "\n",
        "                if best_match is not None:\n",
        "                    indices.append(feature_dict[best_match])\n",
        "                    component_names.append(best_match)\n",
        "                    print(f\"  Using {best_match} as a substitute\")\n",
        "                else:\n",
        "                    # If we still can't find anything, use a dummy index\n",
        "                    # This should not happen in practice\n",
        "                    indices.append(1)  # Use first non-bias term as fallback\n",
        "                    component_names.append(self.feature_names[1])\n",
        "                    print(f\"  Using {self.feature_names[1]} as a fallback\")\n",
        "\n",
        "        # Store the indices and names\n",
        "        self.component_indices = indices\n",
        "        self.selected_features = component_names\n",
        "\n",
        "        print(f\"Selected components:\")\n",
        "        for i, (idx, name) in enumerate(zip(indices, component_names)):\n",
        "            print(f\"{i}: {name} (index {idx})\")\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Train the non-linear regression model.\n",
        "\n",
        "        Args:\n",
        "            X_train (pd.DataFrame): Training features (must include the 6 base features)\n",
        "            y_train (np.ndarray): Training targets (ozone concentration)\n",
        "        \"\"\"\n",
        "        # Check if all required features are present\n",
        "        missing_features = [feat for feat in self.base_features if feat not in X_train.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "\n",
        "        # Extract features in the correct order\n",
        "        X = X_train[self.base_features].values\n",
        "\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.poly.fit_transform(X)\n",
        "\n",
        "        # Get feature names\n",
        "        self.feature_names = self.poly.get_feature_names_out(self.base_features)\n",
        "\n",
        "        # Find the indices of the 12 specific components from the paper\n",
        "        self._find_component_indices()\n",
        "\n",
        "        # Select only the components we want\n",
        "        X_selected = X_poly[:, self.component_indices]\n",
        "\n",
        "        # Train the model on selected components\n",
        "        self.model.fit(X_selected, y_train)\n",
        "\n",
        "        # Store the coefficients for the components\n",
        "        self.feature_importances = {}\n",
        "        for i, idx in enumerate(self.component_indices):\n",
        "            if i == 0:  # Skip the bias term\n",
        "                continue\n",
        "\n",
        "            feature_name = self.selected_features[i]\n",
        "            coef = self.model.coef_[i]\n",
        "            self.feature_importances[feature_name] = coef\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted ozone concentrations\n",
        "        \"\"\"\n",
        "        # Check if model is trained\n",
        "        if not hasattr(self, 'feature_names'):\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        # Check if all required features are present\n",
        "        missing_features = [feat for feat in self.base_features if feat not in X.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "\n",
        "        # Extract features in the correct order\n",
        "        X_feat = X[self.base_features].values\n",
        "\n",
        "        # Generate polynomial features\n",
        "        X_poly = self.poly.transform(X_feat)\n",
        "\n",
        "        # Extract only selected components\n",
        "        X_selected = X_poly[:, self.component_indices]\n",
        "\n",
        "        # Make predictions\n",
        "        return self.model.predict(X_selected)\n",
        "\n",
        "    def evaluate(self, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Extract features for R² calculation\n",
        "        X_feat = X_test[self.base_features].values\n",
        "        X_poly = self.poly.transform(X_feat)\n",
        "        X_selected = X_poly[:, self.component_indices]\n",
        "\n",
        "        self.metrics = {\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'r2': self.model.score(X_selected, y_test),\n",
        "            'avg_predicted': float(np.mean(y_pred)),\n",
        "            'avg_actual': float(np.mean(y_test)),\n",
        "            'n_components': len(self.component_indices) - 1  # Subtract 1 for bias term\n",
        "        }\n",
        "\n",
        "        # Add coefficients to metrics\n",
        "        self.metrics['intercept'] = float(self.model.intercept_)\n",
        "        for i, idx in enumerate(self.component_indices):\n",
        "            if i == 0:  # Skip the bias term\n",
        "                continue\n",
        "\n",
        "            feature_name = self.selected_features[i]\n",
        "            coef = self.model.coef_[i-1]  # -1 because coef doesn't include intercept\n",
        "            self.metrics[f'component_{i}_name'] = feature_name\n",
        "            self.metrics[f'component_{i}_coef'] = float(coef)\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def get_equation_string(self) -> str:\n",
        "        \"\"\"\n",
        "        Get the prediction equation as a formatted string.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted equation string\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'feature_names') or self.component_indices is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        equation = \"[O₃] = \"\n",
        "\n",
        "        # Add intercept\n",
        "        equation += f\"{self.model.intercept_:.1f}\"\n",
        "\n",
        "        # Add each component\n",
        "        for i, idx in enumerate(self.component_indices[1:], 1):  # Start at 1 to skip bias\n",
        "            feature = self.selected_features[i]\n",
        "            coef = self.model.coef_[i-1]  # -1 because coef_ doesn't include intercept\n",
        "\n",
        "            # Format coefficient (handle very small or very large values)\n",
        "            if abs(coef) < 0.001:\n",
        "                coef_str = f\"{coef:.1e}\"\n",
        "            elif abs(coef) < 0.01:\n",
        "                coef_str = f\"{coef:.4f}\"\n",
        "            elif abs(coef) < 0.1:\n",
        "                coef_str = f\"{coef:.3f}\"\n",
        "            elif abs(coef) < 1:\n",
        "                coef_str = f\"{coef:.2f}\"\n",
        "            else:\n",
        "                coef_str = f\"{coef:.1f}\"\n",
        "\n",
        "            # Add term with correct sign\n",
        "            if coef >= 0:\n",
        "                equation += f\" + {coef_str} · {feature}\"\n",
        "            else:\n",
        "                equation += f\" - {abs(coef):.1f} · {feature}\"\n",
        "\n",
        "        # Add units\n",
        "        equation += \" μg m⁻³\"\n",
        "\n",
        "        return equation\n",
        "\n",
        "    def plot_results(self, X_test: pd.DataFrame, y_test: np.ndarray,\n",
        "                    dates_test: pd.DatetimeIndex = None,\n",
        "                    save_dir: str = \"results/figures/nonlinear_exact\") -> None:\n",
        "        \"\"\"\n",
        "        Create and save various plots to analyze model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "            dates_test (pd.DatetimeIndex): Test dates for time series plot\n",
        "            save_dir (str): Directory to save the plots\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Create figures directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Actual vs Predicted scatter plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "\n",
        "        # Add perfect prediction line\n",
        "        max_val = max(np.max(y_test), np.max(y_pred))\n",
        "        min_val = min(np.min(y_test), np.min(y_pred))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "        plt.xlabel('Actual Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.title('Non-linear Model (Paper Components): Actual vs Predicted Ozone')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add metrics text\n",
        "        r2 = self.metrics.get('r2', 0)\n",
        "        rmse = self.metrics.get('rmse', 0)\n",
        "        mae = self.metrics.get('mae', 0)\n",
        "\n",
        "        metrics_text = f\"R² = {r2:.3f}\\nRMSE = {rmse:.2f} µg/m³\\nMAE = {mae:.2f} µg/m³\"\n",
        "        plt.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"actual_vs_predicted.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Residual plot\n",
        "        residuals = y_test - y_pred\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Residuals (µg/m³)')\n",
        "        plt.title('Residual Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add residual statistics\n",
        "        res_mean = np.mean(residuals)\n",
        "        res_std = np.std(residuals)\n",
        "        res_text = f\"Mean = {res_mean:.2f} µg/m³\\nStd Dev = {res_std:.2f} µg/m³\"\n",
        "        plt.annotate(res_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Histogram of residuals\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.xlabel('Residuals (µg/m³)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Residuals')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Time series plot (if dates are provided)\n",
        "        if dates_test is not None:\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_test, y_test, label='Actual', alpha=0.7)\n",
        "            plt.plot(dates_test, y_pred, label='Predicted', alpha=0.7)\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "            plt.title('Ozone Concentration Over Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 5. Feature importance plot\n",
        "        if hasattr(self, 'feature_importances') and self.feature_importances:\n",
        "            # Convert to DataFrame for plotting\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': list(self.feature_importances.keys()),\n",
        "                'Coefficient': list(self.feature_importances.values())\n",
        "            })\n",
        "\n",
        "            # Sort by absolute coefficient value\n",
        "            importance_df['Abs_Coefficient'] = importance_df['Coefficient'].abs()\n",
        "            importance_df = importance_df.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Create a color mapping for positive/negative values\n",
        "            importance_df['sign'] = np.where(importance_df['Coefficient'] >= 0, 'Positive', 'Negative')\n",
        "\n",
        "            # Create barplot with hue parameter\n",
        "            sns.barplot(\n",
        "                x='Coefficient',\n",
        "                y='Feature',\n",
        "                data=importance_df,\n",
        "                hue='sign',  # Use the sign column for coloring\n",
        "                palette={'Positive': 'royalblue', 'Negative': 'crimson'},\n",
        "                legend=False  # Don't show the legend\n",
        "            )\n",
        "\n",
        "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.title('Feature Importance in Non-linear Model (Paper Components)')\n",
        "            plt.xlabel('Coefficient Value')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"feature_importance.png\"))\n",
        "            plt.close()\n",
        "\n",
        "    def save_metrics(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the metrics\n",
        "        \"\"\"\n",
        "        # Add equation string to metrics\n",
        "        self.metrics['equation'] = self.get_equation_string()\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "    def save_model(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model details (not the full model) to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the model details\n",
        "        \"\"\"\n",
        "        model_data = {\n",
        "            'base_features': self.base_features,\n",
        "            'selected_features': self.selected_features,\n",
        "            'component_indices': self.component_indices,\n",
        "            'intercept': float(self.model.intercept_),\n",
        "            'coefficients': [float(coef) for coef in self.model.coef_],\n",
        "            'equation': self.get_equation_string(),\n",
        "            'metrics': self.metrics\n",
        "        }\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(model_data, f, indent=4)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function to display model results in a more visually appealing format.\n",
        "\"\"\"\n",
        "\n",
        "def display_pretty_results(model, metrics, display_components=True):\n",
        "    \"\"\"\n",
        "    Display model results in a nicely formatted way.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        metrics: Dictionary containing model evaluation metrics\n",
        "        display_components: Whether to display component coefficients\n",
        "    \"\"\"\n",
        "    # Terminal formatting codes\n",
        "    BOLD = '\\033[1m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "\n",
        "    # Print title\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}           NON-LINEAR OZONE PREDICTION MODEL          {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "    # Print model information\n",
        "    print(f\"{BOLD}Model Information:{ENDC}\")\n",
        "    print(f\"  Type: Non-linear Regression with Paper-Specified Components\")\n",
        "    print(f\"  Base Features: {', '.join(model.base_features)}\")\n",
        "    print(f\"  Polynomial Degree: 3\")\n",
        "    print(f\"  Number of Components: {metrics['n_components']}\")\n",
        "\n",
        "    # Print key metrics\n",
        "    print(f\"\\n{BOLD}Performance Metrics:{ENDC}\")\n",
        "    print(f\"  {BOLD}MAE:{ENDC}     {GREEN}{metrics['mae']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}RMSE:{ENDC}    {GREEN}{metrics['rmse']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}R²:{ENDC}      {GREEN}{metrics['r2']:.4f}{ENDC}\")\n",
        "    print(f\"  {BOLD}MSE:{ENDC}     {metrics['mse']:.2f}\")\n",
        "\n",
        "    # Print prediction averages\n",
        "    print(f\"\\n{BOLD}Prediction Summary:{ENDC}\")\n",
        "    print(f\"  Average Predicted: {metrics['avg_predicted']:.2f} µg/m³\")\n",
        "    print(f\"  Average Actual:    {metrics['avg_actual']:.2f} µg/m³\")\n",
        "    print(f\"  Difference:        {abs(metrics['avg_predicted'] - metrics['avg_actual']):.2f} µg/m³\")\n",
        "\n",
        "    # Print model equation\n",
        "    print(f\"\\n{BOLD}Model Equation:{ENDC}\")\n",
        "    # Split equation into multiple lines for readability\n",
        "    equation = metrics.get('equation', model.get_equation_string())\n",
        "    equation_parts = equation.split('+')\n",
        "    formatted_equation = equation_parts[0].strip()  # Start with the first part (usually intercept)\n",
        "    line_length = len(formatted_equation)\n",
        "\n",
        "    for part in equation_parts[1:]:\n",
        "        if '·' in part:  # This is a term\n",
        "            term = part.strip()\n",
        "            if line_length + len(term) + 3 > 80:  # Start new line if too long\n",
        "                formatted_equation += f\"\\n  + {term}\"\n",
        "                line_length = 4 + len(term)\n",
        "            else:\n",
        "                formatted_equation += f\" + {term}\"\n",
        "                line_length += 3 + len(term)\n",
        "\n",
        "    # Handle negative terms\n",
        "    formatted_equation = formatted_equation.replace(\"+ -\", \"- \")\n",
        "\n",
        "    # Print the formatted equation\n",
        "    print(f\"  {formatted_equation}\")\n",
        "\n",
        "    # Print component coefficients\n",
        "    if display_components:\n",
        "        print(f\"\\n{BOLD}Component Coefficients:{ENDC}\")\n",
        "        print(f\"  {BOLD}{'Component':<25} {'Coefficient':>15}{ENDC}\")\n",
        "        print(f\"  {'-'*42}\")\n",
        "\n",
        "        # Print intercept\n",
        "        print(f\"  {'Intercept':<25} {metrics['intercept']:>15.4f}\")\n",
        "\n",
        "        # Print all other components\n",
        "        for i in range(1, metrics['n_components'] + 1):\n",
        "            component_name = metrics.get(f'component_{i}_name', f'Component {i}')\n",
        "            component_coef = metrics.get(f'component_{i}_coef', 0.0)\n",
        "\n",
        "            # Color based on coefficient value\n",
        "            if abs(component_coef) < 0.001:\n",
        "                coef_str = f\"{YELLOW}{component_coef:>15.6f}{ENDC}\"\n",
        "            elif component_coef >= 0:\n",
        "                coef_str = f\"{GREEN}{component_coef:>15.4f}{ENDC}\"\n",
        "            else:\n",
        "                coef_str = f\"{RED}{component_coef:>15.4f}{ENDC}\"\n",
        "\n",
        "            print(f\"  {component_name:<25} {coef_str}\")\n",
        "\n",
        "    # Print footer\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}                      END OF REPORT                    {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def nonlinear_model_paper_components():\n",
        "    \"\"\"Main function to train and evaluate the non-linear model with paper components.\"\"\"\n",
        "    # Create results directory\n",
        "    results_dir = \"results\"\n",
        "    model_dir = os.path.join(results_dir, \"models\")\n",
        "    figures_dir = os.path.join(results_dir, \"figures/nonlinear_exact\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # Load processed data with all required features\n",
        "    processed_dir = \"data/processed\"\n",
        "    train_data = pd.read_csv(os.path.join(processed_dir, \"train_data_full.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(processed_dir, \"test_data_full.csv\"))\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # Extract features and targets\n",
        "    feature_cols = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "    X_train = train_data[feature_cols]\n",
        "    y_train = train_data['ozone'].values\n",
        "    X_test = test_data[feature_cols]\n",
        "    y_test = test_data['ozone'].values\n",
        "\n",
        "    # Print basic data statistics\n",
        "    print(\"\\nData Summary:\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Testing samples: {len(X_test)}\")\n",
        "    print(f\"Training date range: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
        "    print(f\"Testing date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
        "\n",
        "    # Print feature statistics\n",
        "    print(\"\\nFeature statistics (training data):\")\n",
        "    print(X_train.describe())\n",
        "\n",
        "    # Print ozone statistics\n",
        "    print(\"\\nOzone statistics:\")\n",
        "    print(f\"Training mean: {np.mean(y_train):.2f} μg/m³\")\n",
        "    print(f\"Training std: {np.std(y_train):.2f} μg/m³\")\n",
        "    print(f\"Test mean: {np.mean(y_test):.2f} μg/m³\")\n",
        "    print(f\"Test std: {np.std(y_test):.2f} μg/m³\")\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = OzoneNonLinearModelExact()\n",
        "    print(\"\\nTraining model with paper-specified components...\")\n",
        "    model.train(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating model...\")\n",
        "    metrics = model.evaluate(X_test, y_test)\n",
        "    print(\"\\nNon-linear Model Evaluation Metrics (Paper Components):\")\n",
        "    print(f\"MAE: {metrics['mae']:.2f} µg m⁻³\")\n",
        "    print(f\"RMSE: {metrics['rmse']:.2f} µg m⁻³\")\n",
        "    print(f\"R²: {metrics['r2']:.3f}\")\n",
        "\n",
        "    print(\"\\nModel Equation:\")\n",
        "    print(model.get_equation_string())\n",
        "\n",
        "    # Print component coefficients\n",
        "    print(\"\\nComponent Coefficients:\")\n",
        "    for i, idx in enumerate(model.component_indices[1:], 1):  # Skip bias term\n",
        "        feature_name = model.selected_features[i]\n",
        "        coef = model.model.coef_[i-1]  # -1 because coef doesn't include intercept\n",
        "        print(f\"{feature_name}: {coef:.6f}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualization plots...\")\n",
        "    model.plot_results(X_test, y_test, test_data['date'])\n",
        "\n",
        "    # Save metrics and model\n",
        "    model.save_metrics(os.path.join(model_dir, \"nonlinear_exact_metrics.json\"))\n",
        "    model.save_model(os.path.join(model_dir, \"nonlinear_exact_model.json\"))\n",
        "\n",
        "    print(\"\\nModel training and evaluation complete!\")\n",
        "\n",
        "    display_pretty_results(model, metrics)\n",
        "\n",
        "nonlinear_model_paper_components()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCysxfWf_wn_",
        "outputId": "b75d15af-5361-4dcf-d7cf-7aab8b651ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Summary:\n",
            "Training samples: 2337\n",
            "Testing samples: 585\n",
            "Training date range: 2016-01-01 00:00:00 to 2022-05-25 00:00:00\n",
            "Testing date range: 2022-05-26 00:00:00 to 2023-12-31 00:00:00\n",
            "\n",
            "Feature statistics (training data):\n",
            "               NO2          NOX          SO2        NMVOC         TEMP  \\\n",
            "count  2337.000000  2337.000000  2337.000000  2337.000000  2337.000000   \n",
            "mean     24.903252    35.485216     1.656739     0.105370    13.349807   \n",
            "std      13.335278    27.443950     1.319634     0.058972     7.041906   \n",
            "min       3.800000     4.100000     0.100000     0.000000    -2.800000   \n",
            "25%      13.900000    16.000000     0.600000     0.100000     7.200000   \n",
            "50%      21.800000    26.000000     1.200000     0.100000    12.800000   \n",
            "75%      34.200000    47.500000     2.400000     0.100000    19.700000   \n",
            "max      79.800000   228.700000     6.500000     0.400000    28.800000   \n",
            "\n",
            "               RAD  \n",
            "count  2337.000000  \n",
            "mean    161.919042  \n",
            "std     100.255408  \n",
            "min       1.900000  \n",
            "25%      75.600000  \n",
            "50%     149.800000  \n",
            "75%     247.700000  \n",
            "max     362.400000  \n",
            "\n",
            "Ozone statistics:\n",
            "Training mean: 90.95 μg/m³\n",
            "Training std: 43.11 μg/m³\n",
            "Test mean: 93.24 μg/m³\n",
            "Test std: 42.29 μg/m³\n",
            "\n",
            "Training model with paper-specified components...\n",
            "Selected components:\n",
            "0: 1 (index 0)\n",
            "1: NO2^3 (index 28)\n",
            "2: NOX (index 2)\n",
            "3: NOX^2 (index 13)\n",
            "4: NOX^3 (index 49)\n",
            "5: TEMP^3 (index 80)\n",
            "6: NO2 TEMP (index 11)\n",
            "7: NO2 RAD (index 12)\n",
            "8: SO2 NMVOC (index 19)\n",
            "9: SO2 TEMP (index 20)\n",
            "10: NMVOC RAD (index 24)\n",
            "11: NOX TEMP (index 16)\n",
            "12: NOX RAD (index 17)\n",
            "\n",
            "Evaluating model...\n",
            "\n",
            "Non-linear Model Evaluation Metrics (Paper Components):\n",
            "MAE: 12.68 µg m⁻³\n",
            "RMSE: 16.45 µg m⁻³\n",
            "R²: 0.849\n",
            "\n",
            "Model Equation:\n",
            "[O₃] = 74.2 + 0.0e+00 · NO2^3 - 0.0 · NOX - 1.5 · NOX^2 + 0.018 · NOX^3 - 0.0 · TEMP^3 + 0.0031 · NO2 TEMP + 0.15 · NO2 RAD + 0.015 · SO2 NMVOC - 17.9 · SO2 TEMP + 0.75 · NMVOC RAD + 0.34 · NOX TEMP - 0.1 · NOX RAD μg m⁻³\n",
            "\n",
            "Component Coefficients:\n",
            "NO2^3: 0.000000\n",
            "NOX: -0.000172\n",
            "NOX^2: -1.463795\n",
            "NOX^3: 0.017971\n",
            "TEMP^3: -0.000043\n",
            "NO2 TEMP: 0.003090\n",
            "NO2 RAD: 0.150048\n",
            "SO2 NMVOC: 0.015476\n",
            "SO2 TEMP: -17.888992\n",
            "NMVOC RAD: 0.750114\n",
            "NOX TEMP: 0.337473\n",
            "NOX RAD: -0.121710\n",
            "\n",
            "Creating visualization plots...\n",
            "\n",
            "Model training and evaluation complete!\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m           NON-LINEAR OZONE PREDICTION MODEL          \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\u001b[1mModel Information:\u001b[0m\n",
            "  Type: Non-linear Regression with Paper-Specified Components\n",
            "  Base Features: NO2, NOX, SO2, NMVOC, TEMP, RAD\n",
            "  Polynomial Degree: 3\n",
            "  Number of Components: 12\n",
            "\n",
            "\u001b[1mPerformance Metrics:\u001b[0m\n",
            "  \u001b[1mMAE:\u001b[0m     \u001b[92m12.68 µg/m³\u001b[0m\n",
            "  \u001b[1mRMSE:\u001b[0m    \u001b[92m16.45 µg/m³\u001b[0m\n",
            "  \u001b[1mR²:\u001b[0m      \u001b[92m0.8487\u001b[0m\n",
            "  \u001b[1mMSE:\u001b[0m     270.56\n",
            "\n",
            "\u001b[1mPrediction Summary:\u001b[0m\n",
            "  Average Predicted: 93.13 µg/m³\n",
            "  Average Actual:    93.24 µg/m³\n",
            "  Difference:        0.11 µg/m³\n",
            "\n",
            "\u001b[1mModel Equation:\u001b[0m\n",
            "  [O₃] = 74.2 + 00 · NO2^3 - 0.0 · NOX - 1.5 · NOX^2\n",
            "  + 0.018 · NOX^3 - 0.0 · TEMP^3 + 0.0031 · NO2 TEMP + 0.15 · NO2 RAD\n",
            "  + 0.015 · SO2 NMVOC - 17.9 · SO2 TEMP + 0.75 · NMVOC RAD\n",
            "  + 0.34 · NOX TEMP - 0.1 · NOX RAD μg m⁻³\n",
            "\n",
            "\u001b[1mComponent Coefficients:\u001b[0m\n",
            "  \u001b[1mComponent                     Coefficient\u001b[0m\n",
            "  ------------------------------------------\n",
            "  Intercept                         74.2193\n",
            "  NO2^3                     \u001b[93m       0.000000\u001b[0m\n",
            "  NOX                       \u001b[93m      -0.000172\u001b[0m\n",
            "  NOX^2                     \u001b[91m        -1.4638\u001b[0m\n",
            "  NOX^3                     \u001b[92m         0.0180\u001b[0m\n",
            "  TEMP^3                    \u001b[93m      -0.000043\u001b[0m\n",
            "  NO2 TEMP                  \u001b[92m         0.0031\u001b[0m\n",
            "  NO2 RAD                   \u001b[92m         0.1500\u001b[0m\n",
            "  SO2 NMVOC                 \u001b[92m         0.0155\u001b[0m\n",
            "  SO2 TEMP                  \u001b[91m       -17.8890\u001b[0m\n",
            "  NMVOC RAD                 \u001b[92m         0.7501\u001b[0m\n",
            "  NOX TEMP                  \u001b[92m         0.3375\u001b[0m\n",
            "  NOX RAD                   \u001b[91m        -0.1217\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m                      END OF REPORT                    \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Recurrent Neural Network model for ozone concentration prediction.\n",
        "Based on Model 8/9 from the research paper.\n",
        "\"\"\"\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "class OzoneRNNModel:\n",
        "    def __init__(self, rnn_type: str = 'lstm', n_features: int = 12,\n",
        "                 sequence_length: int = 1, name: str = \"Model8_RNN\"):\n",
        "        \"\"\"\n",
        "        Initialize the RNN model for ozone prediction.\n",
        "\n",
        "        Args:\n",
        "            rnn_type (str): Type of RNN to use ('lstm', 'gru', or 'simple')\n",
        "            n_features (int): Number of input features\n",
        "            sequence_length (int): Length of input sequences for temporal modeling\n",
        "            name (str): Name of the model\n",
        "        \"\"\"\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.n_features = n_features\n",
        "        self.sequence_length = sequence_length\n",
        "        self.name = name\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = None\n",
        "\n",
        "        # Initialize scalers\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.target_scaler = StandardScaler()\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics = {}\n",
        "        self.history = None\n",
        "\n",
        "        # Feature names (will be set during training)\n",
        "        self.feature_names = []\n",
        "\n",
        "    def _build_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Build the RNN model architecture.\n",
        "        \"\"\"\n",
        "        # Clear previous session to avoid conflicts\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        # Define input shape based on sequence length and number of features\n",
        "        input_shape = (self.sequence_length, self.n_features)\n",
        "\n",
        "        # Create sequential model\n",
        "        model = Sequential(name=self.name)\n",
        "\n",
        "        # Add RNN layer based on the specified type\n",
        "        if self.rnn_type == 'lstm':\n",
        "            model.add(LSTM(64, input_shape=input_shape, return_sequences=True,\n",
        "                          activation='tanh', name='lstm_1'))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(LSTM(32, activation='tanh', name='lstm_2'))\n",
        "        elif self.rnn_type == 'gru':\n",
        "            model.add(GRU(64, input_shape=input_shape, return_sequences=True,\n",
        "                         activation='tanh', name='gru_1'))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(GRU(32, activation='tanh', name='gru_2'))\n",
        "        else:  # simple RNN\n",
        "            model.add(SimpleRNN(64, input_shape=input_shape, return_sequences=True,\n",
        "                              activation='tanh', name='rnn_1'))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(SimpleRNN(32, activation='tanh', name='rnn_2'))\n",
        "\n",
        "        # Add output Dense layers\n",
        "        model.add(Dense(16, activation='relu', name='dense_1'))\n",
        "        model.add(Dense(1, activation='linear', name='output'))\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='mse',\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        # Store model\n",
        "        self.model = model\n",
        "\n",
        "    def _prepare_sequences(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Prepare input sequences for RNN.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Prepared sequences\n",
        "        \"\"\"\n",
        "        # If sequence length is 1, reshape to (samples, 1, features)\n",
        "        if self.sequence_length == 1:\n",
        "            return X.reshape((X.shape[0], self.sequence_length, X.shape[1]))\n",
        "\n",
        "        # Otherwise, create proper sequences\n",
        "        sequences = []\n",
        "        for i in range(len(X) - self.sequence_length + 1):\n",
        "            sequences.append(X[i:i + self.sequence_length])\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: np.ndarray,\n",
        "              validation_split: float = 0.2, epochs: int = 100,\n",
        "              batch_size: int = 32, patience: int = 20) -> None:\n",
        "        \"\"\"\n",
        "        Train the RNN model.\n",
        "\n",
        "        Args:\n",
        "            X_train (pd.DataFrame): Training features\n",
        "            y_train (np.ndarray): Training targets (ozone concentration)\n",
        "            validation_split (float): Fraction of training data to use for validation\n",
        "            epochs (int): Maximum number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            patience (int): Patience for early stopping\n",
        "        \"\"\"\n",
        "        # Store feature names\n",
        "        self.feature_names = X_train.columns.tolist()\n",
        "        self.n_features = len(self.feature_names)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.feature_scaler.fit_transform(X_train)\n",
        "\n",
        "        # Reshape y_train for scaler\n",
        "        y_reshaped = y_train.reshape(-1, 1)\n",
        "        y_scaled = self.target_scaler.fit_transform(y_reshaped).flatten()\n",
        "\n",
        "        # Prepare input sequences\n",
        "        X_sequences = self._prepare_sequences(X_scaled)\n",
        "\n",
        "        # Build the model\n",
        "        self._build_model()\n",
        "\n",
        "        # Print model summary\n",
        "        self.model.summary()\n",
        "\n",
        "        # Set up callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_sequences, y_scaled,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted ozone concentrations\n",
        "        \"\"\"\n",
        "        # Check if model is built\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        # Check feature order\n",
        "        missing_features = [feat for feat in self.feature_names if feat not in X.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing features: {missing_features}\")\n",
        "\n",
        "        # Ensure correct feature order\n",
        "        X = X[self.feature_names]\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.feature_scaler.transform(X)\n",
        "\n",
        "        # Prepare sequences\n",
        "        X_sequences = self._prepare_sequences(X_scaled)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_scaled = self.model.predict(X_sequences)\n",
        "\n",
        "        # Inverse transform to get original scale\n",
        "        y_pred = self.target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def evaluate(self, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        # Make predictions\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics = {\n",
        "            'mae': float(mae),\n",
        "            'mse': float(mse),\n",
        "            'rmse': float(rmse),\n",
        "            'r2': float(r2),\n",
        "            'avg_predicted': float(np.mean(y_pred)),\n",
        "            'avg_actual': float(np.mean(y_test)),\n",
        "            'rnn_type': self.rnn_type,\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'n_features': self.n_features\n",
        "        }\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_results(self, X_test: pd.DataFrame, y_test: np.ndarray,\n",
        "                    dates_test: pd.DatetimeIndex = None,\n",
        "                    save_dir: str = \"results/figures/rnn\") -> None:\n",
        "        \"\"\"\n",
        "        Create and save plots to visualize model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "            dates_test (pd.DatetimeIndex): Test dates for time series plot\n",
        "            save_dir (str): Directory to save the plots\n",
        "        \"\"\"\n",
        "        # Create save directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # 1. Training history plot\n",
        "        if self.history is not None:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            # Plot training & validation loss\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "            plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss During Training')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "\n",
        "            # Plot training & validation MAE\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(self.history.history['mae'], label='Training MAE')\n",
        "            plt.plot(self.history.history['val_mae'], label='Validation MAE')\n",
        "            plt.title('Model MAE During Training')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"training_history.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 2. Actual vs Predicted scatter plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "\n",
        "        # Add perfect prediction line\n",
        "        max_val = max(np.max(y_test), np.max(y_pred))\n",
        "        min_val = min(np.min(y_test), np.min(y_pred))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "        plt.xlabel('Actual Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.title(f'{self.rnn_type.upper()} Model: Actual vs Predicted Ozone')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add metrics text\n",
        "        r2 = self.metrics.get('r2', 0)\n",
        "        rmse = self.metrics.get('rmse', 0)\n",
        "        mae = self.metrics.get('mae', 0)\n",
        "\n",
        "        metrics_text = f\"R² = {r2:.3f}\\nRMSE = {rmse:.2f} µg/m³\\nMAE = {mae:.2f} µg/m³\"\n",
        "        plt.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"actual_vs_predicted.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Residual plot\n",
        "        residuals = y_test - y_pred\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Residuals (µg/m³)')\n",
        "        plt.title('Residual Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add residual statistics\n",
        "        res_mean = np.mean(residuals)\n",
        "        res_std = np.std(residuals)\n",
        "        res_text = f\"Mean = {res_mean:.2f} µg/m³\\nStd Dev = {res_std:.2f} µg/m³\"\n",
        "        plt.annotate(res_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Time series plot (if dates are provided)\n",
        "        if dates_test is not None:\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_test, y_test, label='Actual', alpha=0.7)\n",
        "            plt.plot(dates_test, y_pred, label='Predicted', alpha=0.7)\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "            plt.title('Ozone Concentration Over Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 5. Residuals over time\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_test, residuals, color='royalblue', alpha=0.7)\n",
        "            plt.axhline(y=0, color='r', linestyle='--')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Residuals (µg/m³)')\n",
        "            plt.title('Prediction Residuals Over Time')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"residuals_time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 6. Histogram of residuals\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.xlabel('Residuals (µg/m³)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Residuals')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def save_metrics(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the metrics\n",
        "        \"\"\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "    def save_model(self, save_dir: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained model.\n",
        "\n",
        "        Args:\n",
        "            save_dir (str): Directory to save the model\n",
        "        \"\"\"\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Create model name\n",
        "        model_path = os.path.join(save_dir, f\"{self.name}.h5\")\n",
        "\n",
        "        # Save model\n",
        "        self.model.save(model_path)\n",
        "\n",
        "        # Save scalers\n",
        "        import pickle\n",
        "        with open(os.path.join(save_dir, f\"{self.name}_feature_scaler.pkl\"), 'wb') as f:\n",
        "            pickle.dump(self.feature_scaler, f)\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{self.name}_target_scaler.pkl\"), 'wb') as f:\n",
        "            pickle.dump(self.target_scaler, f)\n",
        "\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_dir: str, model_name: str) -> 'OzoneRNNModel':\n",
        "        \"\"\"\n",
        "        Load a saved model.\n",
        "\n",
        "        Args:\n",
        "            model_dir (str): Directory where the model is saved\n",
        "            model_name (str): Name of the model\n",
        "\n",
        "        Returns:\n",
        "            OzoneRNNModel: Loaded model\n",
        "        \"\"\"\n",
        "        # Create model path\n",
        "        model_path = os.path.join(model_dir, f\"{model_name}.h5\")\n",
        "\n",
        "        # Load model\n",
        "        loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "        # Load scalers\n",
        "        import pickle\n",
        "        with open(os.path.join(model_dir, f\"{model_name}_feature_scaler.pkl\"), 'rb') as f:\n",
        "            feature_scaler = pickle.load(f)\n",
        "\n",
        "        with open(os.path.join(model_dir, f\"{model_name}_target_scaler.pkl\"), 'rb') as f:\n",
        "            target_scaler = pickle.load(f)\n",
        "\n",
        "        # Create new instance\n",
        "        instance = cls(name=model_name)\n",
        "        instance.model = loaded_model\n",
        "        instance.feature_scaler = feature_scaler\n",
        "        instance.target_scaler = target_scaler\n",
        "\n",
        "        # Try to determine RNN type from model layers\n",
        "        for layer in loaded_model.layers:\n",
        "            if 'lstm' in layer.name:\n",
        "                instance.rnn_type = 'lstm'\n",
        "                break\n",
        "            elif 'gru' in layer.name:\n",
        "                instance.rnn_type = 'gru'\n",
        "                break\n",
        "            elif 'rnn' in layer.name:\n",
        "                instance.rnn_type = 'simple'\n",
        "                break\n",
        "\n",
        "        return instance\n",
        "\n",
        "def display_pretty_results(model, metrics, display_components=True):\n",
        "    \"\"\"\n",
        "    Display model results in a nicely formatted way.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        metrics: Dictionary containing model evaluation metrics\n",
        "        display_components: Whether to display component coefficients\n",
        "    \"\"\"\n",
        "    # Terminal formatting codes\n",
        "    BOLD = '\\033[1m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "\n",
        "    # Get RNN type\n",
        "    rnn_type = metrics.get('rnn_type', 'RNN').upper()\n",
        "\n",
        "    # Print title\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}           {rnn_type} OZONE PREDICTION MODEL            {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "    # Print model information\n",
        "    print(f\"{BOLD}Model Information:{ENDC}\")\n",
        "    print(f\"  Type: {rnn_type} Neural Network\")\n",
        "    print(f\"  Features: {metrics.get('n_features', 'Unknown')}\")\n",
        "\n",
        "    if 'sequence_length' in metrics:\n",
        "        print(f\"  Sequence Length: {metrics['sequence_length']}\")\n",
        "\n",
        "    # Print key metrics\n",
        "    print(f\"\\n{BOLD}Performance Metrics:{ENDC}\")\n",
        "    print(f\"  {BOLD}MAE:{ENDC}     {GREEN}{metrics['mae']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}RMSE:{ENDC}    {GREEN}{metrics['rmse']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}R²:{ENDC}      {GREEN}{metrics['r2']:.4f}{ENDC}\")\n",
        "    print(f\"  {BOLD}MSE:{ENDC}     {metrics['mse']:.2f}\")\n",
        "\n",
        "    # Print prediction averages\n",
        "    print(f\"\\n{BOLD}Prediction Summary:{ENDC}\")\n",
        "    print(f\"  Average Predicted: {metrics['avg_predicted']:.2f} µg/m³\")\n",
        "    print(f\"  Average Actual:    {metrics['avg_actual']:.2f} µg/m³\")\n",
        "    print(f\"  Difference:        {abs(metrics['avg_predicted'] - metrics['avg_actual']):.2f} µg/m³\")\n",
        "\n",
        "    # Print model structure if available\n",
        "    if hasattr(model, 'model') and model.model is not None:\n",
        "        print(f\"\\n{BOLD}Model Structure:{ENDC}\")\n",
        "        model.model.summary(print_fn=lambda x: print(f\"  {x}\"))\n",
        "\n",
        "    # Print footer\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}                      END OF REPORT                    {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "\n",
        "def rnn_ozone_model():\n",
        "    \"\"\"Main function to train and evaluate the RNN model.\"\"\"\n",
        "    # Create results directory\n",
        "    results_dir = \"results\"\n",
        "    model_dir = os.path.join(results_dir, \"models\")\n",
        "    figures_dir = os.path.join(results_dir, \"figures/rnn\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # Load processed data with all required features\n",
        "    processed_dir = \"data/processed\"\n",
        "    train_data = pd.read_csv(os.path.join(processed_dir, \"train_data_full.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(processed_dir, \"test_data_full.csv\"))\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # Extract features and targets\n",
        "    feature_cols = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "    # Add derived features like ones in the paper\n",
        "    train_data['DayOfYear_Sin'] = np.sin(2 * np.pi * train_data['date'].dt.dayofyear / 365)\n",
        "    train_data['DayOfYear_Cos'] = np.cos(2 * np.pi * train_data['date'].dt.dayofyear / 365)\n",
        "\n",
        "    test_data['DayOfYear_Sin'] = np.sin(2 * np.pi * test_data['date'].dt.dayofyear / 365)\n",
        "    test_data['DayOfYear_Cos'] = np.cos(2 * np.pi * test_data['date'].dt.dayofyear / 365)\n",
        "\n",
        "    # Extended feature list with seasonality\n",
        "    feature_cols_extended = feature_cols + ['DayOfYear_Sin', 'DayOfYear_Cos']\n",
        "\n",
        "    X_train = train_data[feature_cols_extended]\n",
        "    y_train = train_data['ozone'].values\n",
        "    X_test = test_data[feature_cols_extended]\n",
        "    y_test = test_data['ozone'].values\n",
        "\n",
        "    # Print dataset information\n",
        "    print(\"\\nDataset Information:\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Testing samples: {len(X_test)}\")\n",
        "    print(f\"Features: {', '.join(feature_cols_extended)}\")\n",
        "\n",
        "    # Train LSTM model (as in Model 8/9 from the paper)\n",
        "    print(\"\\nTraining LSTM model...\")\n",
        "    lstm_model = OzoneRNNModel(rnn_type='lstm', n_features=len(feature_cols_extended),\n",
        "                              sequence_length=1, name=\"Model8_LSTM\")\n",
        "\n",
        "    lstm_model.train(X_train, y_train, validation_split=0.2,\n",
        "                    epochs=150, batch_size=32, patience=30)\n",
        "\n",
        "    # Evaluate LSTM model\n",
        "    print(\"\\nEvaluating LSTM model...\")\n",
        "    lstm_metrics = lstm_model.evaluate(X_test, y_test)\n",
        "\n",
        "    # Display pretty results\n",
        "    display_pretty_results(lstm_model, lstm_metrics)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualization plots...\")\n",
        "    lstm_model.plot_results(X_test, y_test, test_data['date'],\n",
        "                           save_dir=os.path.join(figures_dir, \"lstm\"))\n",
        "\n",
        "    # Save model and metrics\n",
        "    lstm_model.save_metrics(os.path.join(model_dir, \"lstm_metrics.json\"))\n",
        "    lstm_model.save_model(model_dir)\n",
        "\n",
        "    # Try GRU model as well (for comparison)\n",
        "    print(\"\\nTraining GRU model...\")\n",
        "    gru_model = OzoneRNNModel(rnn_type='gru', n_features=len(feature_cols_extended),\n",
        "                             sequence_length=1, name=\"Model9_GRU\")\n",
        "\n",
        "    gru_model.train(X_train, y_train, validation_split=0.2,\n",
        "                   epochs=150, batch_size=32, patience=30)\n",
        "\n",
        "    # Evaluate GRU model\n",
        "    print(\"\\nEvaluating GRU model...\")\n",
        "    gru_metrics = gru_model.evaluate(X_test, y_test)\n",
        "\n",
        "    # Display pretty results\n",
        "    display_pretty_results(gru_model, gru_metrics)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualization plots...\")\n",
        "    gru_model.plot_results(X_test, y_test, test_data['date'],\n",
        "                          save_dir=os.path.join(figures_dir, \"gru\"))\n",
        "\n",
        "    # Save model and metrics\n",
        "    gru_model.save_metrics(os.path.join(model_dir, \"gru_metrics.json\"))\n",
        "    gru_model.save_model(model_dir)\n",
        "\n",
        "    # Compare models\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    comparison = {\n",
        "        'LSTM': lstm_metrics,\n",
        "        'GRU': gru_metrics\n",
        "    }\n",
        "\n",
        "    for model_name, metrics in comparison.items():\n",
        "        print(f\"{model_name}: MAE = {metrics['mae']:.2f} µg/m³, RMSE = {metrics['rmse']:.2f} µg/m³, R² = {metrics['r2']:.4f}\")\n",
        "\n",
        "    # Return the best model\n",
        "    if lstm_metrics['mae'] <= gru_metrics['mae']:\n",
        "        print(f\"\\nBest model: LSTM with MAE = {lstm_metrics['mae']:.2f} µg/m³\")\n",
        "        return lstm_model, lstm_metrics\n",
        "    else:\n",
        "        print(f\"\\nBest model: GRU with MAE = {gru_metrics['mae']:.2f} µg/m³\")\n",
        "        return gru_model, gru_metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rnn_ozone_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIFmk9LZMyWB",
        "outputId": "00474c7d-2d50-4da4-8b58-9a9b8d3aeea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Information:\n",
            "Training samples: 2337\n",
            "Testing samples: 585\n",
            "Features: NO2, NOX, SO2, NMVOC, TEMP, RAD, DayOfYear_Sin, DayOfYear_Cos\n",
            "\n",
            "Training LSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Model8_LSTM\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model8_LSTM\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m18,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,649\u001b[0m (123.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,649</span> (123.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,649\u001b[0m (123.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,649</span> (123.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.9539 - mae: 0.7920 - val_loss: 0.3760 - val_mae: 0.4939 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3080 - mae: 0.4492 - val_loss: 0.1658 - val_mae: 0.3264 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1820 - mae: 0.3389 - val_loss: 0.1487 - val_mae: 0.3059 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1730 - mae: 0.3261 - val_loss: 0.1375 - val_mae: 0.2931 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1502 - mae: 0.3045 - val_loss: 0.1435 - val_mae: 0.2946 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1411 - mae: 0.2938 - val_loss: 0.1283 - val_mae: 0.2788 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1345 - mae: 0.2866 - val_loss: 0.1211 - val_mae: 0.2713 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1371 - mae: 0.2867 - val_loss: 0.1075 - val_mae: 0.2554 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1410 - mae: 0.2895 - val_loss: 0.1150 - val_mae: 0.2642 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1334 - mae: 0.2844 - val_loss: 0.1278 - val_mae: 0.2768 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1264 - mae: 0.2788 - val_loss: 0.1122 - val_mae: 0.2604 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1265 - mae: 0.2750 - val_loss: 0.1042 - val_mae: 0.2491 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1292 - mae: 0.2754 - val_loss: 0.1146 - val_mae: 0.2607 - learning_rate: 0.0010\n",
            "Epoch 14/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1228 - mae: 0.2744 - val_loss: 0.1095 - val_mae: 0.2565 - learning_rate: 0.0010\n",
            "Epoch 15/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1232 - mae: 0.2761 - val_loss: 0.1119 - val_mae: 0.2563 - learning_rate: 0.0010\n",
            "Epoch 16/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1249 - mae: 0.2748 - val_loss: 0.1172 - val_mae: 0.2628 - learning_rate: 0.0010\n",
            "Epoch 17/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1206 - mae: 0.2732 - val_loss: 0.1165 - val_mae: 0.2616 - learning_rate: 0.0010\n",
            "Epoch 18/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1275 - mae: 0.2743 - val_loss: 0.1050 - val_mae: 0.2511 - learning_rate: 0.0010\n",
            "Epoch 19/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1153 - mae: 0.2643 - val_loss: 0.1094 - val_mae: 0.2529 - learning_rate: 0.0010\n",
            "Epoch 20/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1164 - mae: 0.2607 - val_loss: 0.1236 - val_mae: 0.2678 - learning_rate: 0.0010\n",
            "Epoch 21/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1173 - mae: 0.2694 - val_loss: 0.1194 - val_mae: 0.2647 - learning_rate: 0.0010\n",
            "Epoch 22/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1094 - mae: 0.2578 - val_loss: 0.1142 - val_mae: 0.2583 - learning_rate: 0.0010\n",
            "Epoch 23/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1185 - mae: 0.2684 - val_loss: 0.1039 - val_mae: 0.2481 - learning_rate: 5.0000e-04\n",
            "Epoch 24/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1229 - mae: 0.2748 - val_loss: 0.1063 - val_mae: 0.2490 - learning_rate: 5.0000e-04\n",
            "Epoch 25/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1222 - mae: 0.2700 - val_loss: 0.1049 - val_mae: 0.2472 - learning_rate: 5.0000e-04\n",
            "Epoch 26/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1020 - mae: 0.2497 - val_loss: 0.1093 - val_mae: 0.2543 - learning_rate: 5.0000e-04\n",
            "Epoch 27/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1104 - mae: 0.2609 - val_loss: 0.1039 - val_mae: 0.2451 - learning_rate: 5.0000e-04\n",
            "Epoch 28/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1201 - mae: 0.2704 - val_loss: 0.1043 - val_mae: 0.2470 - learning_rate: 5.0000e-04\n",
            "Epoch 29/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1081 - mae: 0.2604 - val_loss: 0.1039 - val_mae: 0.2475 - learning_rate: 5.0000e-04\n",
            "Epoch 30/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1140 - mae: 0.2648 - val_loss: 0.1089 - val_mae: 0.2522 - learning_rate: 5.0000e-04\n",
            "Epoch 31/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1153 - mae: 0.2649 - val_loss: 0.1084 - val_mae: 0.2519 - learning_rate: 5.0000e-04\n",
            "Epoch 32/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1110 - mae: 0.2623 - val_loss: 0.1038 - val_mae: 0.2479 - learning_rate: 5.0000e-04\n",
            "Epoch 33/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1231 - mae: 0.2771 - val_loss: 0.1008 - val_mae: 0.2428 - learning_rate: 5.0000e-04\n",
            "Epoch 34/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1116 - mae: 0.2550 - val_loss: 0.1014 - val_mae: 0.2433 - learning_rate: 5.0000e-04\n",
            "Epoch 35/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1139 - mae: 0.2657 - val_loss: 0.0966 - val_mae: 0.2372 - learning_rate: 5.0000e-04\n",
            "Epoch 36/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1179 - mae: 0.2666 - val_loss: 0.1070 - val_mae: 0.2509 - learning_rate: 5.0000e-04\n",
            "Epoch 37/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1181 - mae: 0.2680 - val_loss: 0.1079 - val_mae: 0.2503 - learning_rate: 5.0000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1185 - mae: 0.2680 - val_loss: 0.1056 - val_mae: 0.2476 - learning_rate: 5.0000e-04\n",
            "Epoch 39/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1162 - mae: 0.2636 - val_loss: 0.1026 - val_mae: 0.2441 - learning_rate: 5.0000e-04\n",
            "Epoch 40/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1038 - mae: 0.2503 - val_loss: 0.1067 - val_mae: 0.2495 - learning_rate: 5.0000e-04\n",
            "Epoch 41/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1130 - mae: 0.2588 - val_loss: 0.1008 - val_mae: 0.2416 - learning_rate: 5.0000e-04\n",
            "Epoch 42/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1080 - mae: 0.2571 - val_loss: 0.1065 - val_mae: 0.2500 - learning_rate: 5.0000e-04\n",
            "Epoch 43/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1147 - mae: 0.2653 - val_loss: 0.1039 - val_mae: 0.2453 - learning_rate: 5.0000e-04\n",
            "Epoch 44/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1065 - mae: 0.2528 - val_loss: 0.1006 - val_mae: 0.2413 - learning_rate: 5.0000e-04\n",
            "Epoch 45/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1095 - mae: 0.2539 - val_loss: 0.1124 - val_mae: 0.2549 - learning_rate: 5.0000e-04\n",
            "Epoch 46/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1143 - mae: 0.2622 - val_loss: 0.1055 - val_mae: 0.2485 - learning_rate: 2.5000e-04\n",
            "Epoch 47/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1138 - mae: 0.2588 - val_loss: 0.0977 - val_mae: 0.2381 - learning_rate: 2.5000e-04\n",
            "Epoch 48/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1084 - mae: 0.2564 - val_loss: 0.1005 - val_mae: 0.2424 - learning_rate: 2.5000e-04\n",
            "Epoch 49/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1147 - mae: 0.2646 - val_loss: 0.1054 - val_mae: 0.2473 - learning_rate: 2.5000e-04\n",
            "Epoch 50/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1092 - mae: 0.2560 - val_loss: 0.1015 - val_mae: 0.2434 - learning_rate: 2.5000e-04\n",
            "Epoch 51/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1078 - mae: 0.2545 - val_loss: 0.1034 - val_mae: 0.2457 - learning_rate: 2.5000e-04\n",
            "Epoch 52/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1124 - mae: 0.2582 - val_loss: 0.0997 - val_mae: 0.2413 - learning_rate: 2.5000e-04\n",
            "Epoch 53/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1147 - mae: 0.2586 - val_loss: 0.1012 - val_mae: 0.2432 - learning_rate: 2.5000e-04\n",
            "Epoch 54/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1093 - mae: 0.2553 - val_loss: 0.1053 - val_mae: 0.2479 - learning_rate: 2.5000e-04\n",
            "Epoch 55/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1113 - mae: 0.2591 - val_loss: 0.1009 - val_mae: 0.2428 - learning_rate: 2.5000e-04\n",
            "Epoch 56/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1057 - mae: 0.2548 - val_loss: 0.1025 - val_mae: 0.2441 - learning_rate: 1.2500e-04\n",
            "Epoch 57/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1077 - mae: 0.2565 - val_loss: 0.1019 - val_mae: 0.2443 - learning_rate: 1.2500e-04\n",
            "Epoch 58/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1157 - mae: 0.2647 - val_loss: 0.1048 - val_mae: 0.2465 - learning_rate: 1.2500e-04\n",
            "Epoch 59/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1158 - mae: 0.2610 - val_loss: 0.1027 - val_mae: 0.2443 - learning_rate: 1.2500e-04\n",
            "Epoch 60/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1104 - mae: 0.2582 - val_loss: 0.1022 - val_mae: 0.2444 - learning_rate: 1.2500e-04\n",
            "Epoch 61/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1135 - mae: 0.2592 - val_loss: 0.1045 - val_mae: 0.2470 - learning_rate: 1.2500e-04\n",
            "Epoch 62/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1195 - mae: 0.2705 - val_loss: 0.1031 - val_mae: 0.2450 - learning_rate: 1.2500e-04\n",
            "Epoch 63/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1104 - mae: 0.2576 - val_loss: 0.1002 - val_mae: 0.2411 - learning_rate: 1.2500e-04\n",
            "Epoch 64/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1086 - mae: 0.2544 - val_loss: 0.1018 - val_mae: 0.2435 - learning_rate: 1.2500e-04\n",
            "Epoch 65/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1122 - mae: 0.2597 - val_loss: 0.1047 - val_mae: 0.2472 - learning_rate: 1.2500e-04\n",
            "\n",
            "Evaluating LSTM model...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m           LSTM OZONE PREDICTION MODEL            \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\u001b[1mModel Information:\u001b[0m\n",
            "  Type: LSTM Neural Network\n",
            "  Features: 8\n",
            "  Sequence Length: 1\n",
            "\n",
            "\u001b[1mPerformance Metrics:\u001b[0m\n",
            "  \u001b[1mMAE:\u001b[0m     \u001b[92m13.51 µg/m³\u001b[0m\n",
            "  \u001b[1mRMSE:\u001b[0m    \u001b[92m17.12 µg/m³\u001b[0m\n",
            "  \u001b[1mR²:\u001b[0m      \u001b[92m0.8361\u001b[0m\n",
            "  \u001b[1mMSE:\u001b[0m     293.17\n",
            "\n",
            "\u001b[1mPrediction Summary:\u001b[0m\n",
            "  Average Predicted: 95.88 µg/m³\n",
            "  Average Actual:    93.24 µg/m³\n",
            "  Difference:        2.64 µg/m³\n",
            "\n",
            "\u001b[1mModel Structure:\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model: \"Model8_LSTM\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ lstm_1 (LSTM)                   │ (None, 1, 64)          │        18,688 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout (Dropout)               │ (None, 1, 64)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ lstm_2 (LSTM)                   │ (None, 32)             │        12,416 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_1 (Dense)                 │ (None, 16)             │           528 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ output (Dense)                  │ (None, 1)              │            17 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 94,949 (370.90 KB)\n",
            " Trainable params: 31,649 (123.63 KB)\n",
            " Non-trainable params: 0 (0.00 B)\n",
            " Optimizer params: 63,300 (247.27 KB)\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m                      END OF REPORT                    \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\n",
            "Creating visualization plots...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to results/models/Model8_LSTM.h5\n",
            "\n",
            "Training GRU model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Model9_GRU\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model9_GRU\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m14,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m9,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,161\u001b[0m (94.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,161</span> (94.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,161\u001b[0m (94.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,161</span> (94.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.5886 - mae: 0.6059 - val_loss: 0.1808 - val_mae: 0.3422 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1904 - mae: 0.3495 - val_loss: 0.1481 - val_mae: 0.3048 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1683 - mae: 0.3220 - val_loss: 0.1300 - val_mae: 0.2855 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1602 - mae: 0.3142 - val_loss: 0.1251 - val_mae: 0.2782 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1405 - mae: 0.2951 - val_loss: 0.1199 - val_mae: 0.2705 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1358 - mae: 0.2852 - val_loss: 0.1196 - val_mae: 0.2685 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1435 - mae: 0.2939 - val_loss: 0.1160 - val_mae: 0.2641 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1377 - mae: 0.2865 - val_loss: 0.1279 - val_mae: 0.2747 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1295 - mae: 0.2797 - val_loss: 0.1102 - val_mae: 0.2559 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1284 - mae: 0.2801 - val_loss: 0.1163 - val_mae: 0.2620 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1278 - mae: 0.2786 - val_loss: 0.1083 - val_mae: 0.2537 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1253 - mae: 0.2738 - val_loss: 0.1234 - val_mae: 0.2709 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1230 - mae: 0.2707 - val_loss: 0.1171 - val_mae: 0.2625 - learning_rate: 0.0010\n",
            "Epoch 14/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1251 - mae: 0.2794 - val_loss: 0.1110 - val_mae: 0.2553 - learning_rate: 0.0010\n",
            "Epoch 15/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1263 - mae: 0.2750 - val_loss: 0.1179 - val_mae: 0.2614 - learning_rate: 0.0010\n",
            "Epoch 16/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1199 - mae: 0.2668 - val_loss: 0.1217 - val_mae: 0.2669 - learning_rate: 0.0010\n",
            "Epoch 17/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1172 - mae: 0.2641 - val_loss: 0.1200 - val_mae: 0.2630 - learning_rate: 0.0010\n",
            "Epoch 18/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1217 - mae: 0.2693 - val_loss: 0.1241 - val_mae: 0.2684 - learning_rate: 0.0010\n",
            "Epoch 19/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1179 - mae: 0.2710 - val_loss: 0.1122 - val_mae: 0.2533 - learning_rate: 0.0010\n",
            "Epoch 20/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1237 - mae: 0.2722 - val_loss: 0.1129 - val_mae: 0.2568 - learning_rate: 0.0010\n",
            "Epoch 21/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1255 - mae: 0.2784 - val_loss: 0.1395 - val_mae: 0.2857 - learning_rate: 0.0010\n",
            "Epoch 22/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1215 - mae: 0.2741 - val_loss: 0.1152 - val_mae: 0.2578 - learning_rate: 5.0000e-04\n",
            "Epoch 23/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1134 - mae: 0.2616 - val_loss: 0.1079 - val_mae: 0.2493 - learning_rate: 5.0000e-04\n",
            "Epoch 24/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1107 - mae: 0.2551 - val_loss: 0.1198 - val_mae: 0.2627 - learning_rate: 5.0000e-04\n",
            "Epoch 25/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1178 - mae: 0.2671 - val_loss: 0.1064 - val_mae: 0.2500 - learning_rate: 5.0000e-04\n",
            "Epoch 26/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1129 - mae: 0.2612 - val_loss: 0.1099 - val_mae: 0.2524 - learning_rate: 5.0000e-04\n",
            "Epoch 27/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1119 - mae: 0.2564 - val_loss: 0.1113 - val_mae: 0.2563 - learning_rate: 5.0000e-04\n",
            "Epoch 28/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1142 - mae: 0.2618 - val_loss: 0.1121 - val_mae: 0.2545 - learning_rate: 5.0000e-04\n",
            "Epoch 29/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1200 - mae: 0.2657 - val_loss: 0.1062 - val_mae: 0.2482 - learning_rate: 5.0000e-04\n",
            "Epoch 30/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1023 - mae: 0.2508 - val_loss: 0.1083 - val_mae: 0.2517 - learning_rate: 5.0000e-04\n",
            "Epoch 31/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1138 - mae: 0.2557 - val_loss: 0.1110 - val_mae: 0.2542 - learning_rate: 5.0000e-04\n",
            "Epoch 32/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1174 - mae: 0.2679 - val_loss: 0.1114 - val_mae: 0.2544 - learning_rate: 5.0000e-04\n",
            "Epoch 33/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1052 - mae: 0.2490 - val_loss: 0.1052 - val_mae: 0.2479 - learning_rate: 5.0000e-04\n",
            "Epoch 34/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1123 - mae: 0.2594 - val_loss: 0.1053 - val_mae: 0.2474 - learning_rate: 5.0000e-04\n",
            "Epoch 35/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1124 - mae: 0.2628 - val_loss: 0.1025 - val_mae: 0.2436 - learning_rate: 5.0000e-04\n",
            "Epoch 36/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1095 - mae: 0.2570 - val_loss: 0.1073 - val_mae: 0.2510 - learning_rate: 5.0000e-04\n",
            "Epoch 37/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1166 - mae: 0.2603 - val_loss: 0.1112 - val_mae: 0.2561 - learning_rate: 5.0000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1117 - mae: 0.2605 - val_loss: 0.1047 - val_mae: 0.2490 - learning_rate: 5.0000e-04\n",
            "Epoch 39/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1079 - mae: 0.2585 - val_loss: 0.1155 - val_mae: 0.2599 - learning_rate: 5.0000e-04\n",
            "Epoch 40/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1104 - mae: 0.2606 - val_loss: 0.1077 - val_mae: 0.2515 - learning_rate: 5.0000e-04\n",
            "Epoch 41/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1058 - mae: 0.2504 - val_loss: 0.1054 - val_mae: 0.2483 - learning_rate: 5.0000e-04\n",
            "Epoch 42/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1099 - mae: 0.2595 - val_loss: 0.1120 - val_mae: 0.2551 - learning_rate: 5.0000e-04\n",
            "Epoch 43/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1162 - mae: 0.2632 - val_loss: 0.1067 - val_mae: 0.2492 - learning_rate: 5.0000e-04\n",
            "Epoch 44/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1165 - mae: 0.2614 - val_loss: 0.1176 - val_mae: 0.2603 - learning_rate: 5.0000e-04\n",
            "Epoch 45/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1146 - mae: 0.2616 - val_loss: 0.1089 - val_mae: 0.2526 - learning_rate: 5.0000e-04\n",
            "Epoch 46/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1100 - mae: 0.2561 - val_loss: 0.1053 - val_mae: 0.2476 - learning_rate: 2.5000e-04\n",
            "Epoch 47/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1070 - mae: 0.2502 - val_loss: 0.1119 - val_mae: 0.2558 - learning_rate: 2.5000e-04\n",
            "Epoch 48/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1105 - mae: 0.2581 - val_loss: 0.1166 - val_mae: 0.2604 - learning_rate: 2.5000e-04\n",
            "Epoch 49/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1050 - mae: 0.2539 - val_loss: 0.1135 - val_mae: 0.2581 - learning_rate: 2.5000e-04\n",
            "Epoch 50/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1106 - mae: 0.2520 - val_loss: 0.1118 - val_mae: 0.2558 - learning_rate: 2.5000e-04\n",
            "Epoch 51/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1085 - mae: 0.2573 - val_loss: 0.1164 - val_mae: 0.2605 - learning_rate: 2.5000e-04\n",
            "Epoch 52/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1106 - mae: 0.2577 - val_loss: 0.1103 - val_mae: 0.2546 - learning_rate: 2.5000e-04\n",
            "Epoch 53/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0985 - mae: 0.2463 - val_loss: 0.1113 - val_mae: 0.2546 - learning_rate: 2.5000e-04\n",
            "Epoch 54/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1084 - mae: 0.2519 - val_loss: 0.1090 - val_mae: 0.2535 - learning_rate: 2.5000e-04\n",
            "Epoch 55/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1080 - mae: 0.2566 - val_loss: 0.1065 - val_mae: 0.2495 - learning_rate: 2.5000e-04\n",
            "Epoch 56/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0986 - mae: 0.2438 - val_loss: 0.1095 - val_mae: 0.2529 - learning_rate: 1.2500e-04\n",
            "Epoch 57/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1157 - mae: 0.2645 - val_loss: 0.1124 - val_mae: 0.2560 - learning_rate: 1.2500e-04\n",
            "Epoch 58/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1125 - mae: 0.2594 - val_loss: 0.1084 - val_mae: 0.2514 - learning_rate: 1.2500e-04\n",
            "Epoch 59/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1051 - mae: 0.2525 - val_loss: 0.1087 - val_mae: 0.2518 - learning_rate: 1.2500e-04\n",
            "Epoch 60/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0957 - mae: 0.2400 - val_loss: 0.1113 - val_mae: 0.2552 - learning_rate: 1.2500e-04\n",
            "Epoch 61/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1058 - mae: 0.2522 - val_loss: 0.1076 - val_mae: 0.2510 - learning_rate: 1.2500e-04\n",
            "Epoch 62/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1146 - mae: 0.2615 - val_loss: 0.1104 - val_mae: 0.2545 - learning_rate: 1.2500e-04\n",
            "Epoch 63/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1087 - mae: 0.2552 - val_loss: 0.1108 - val_mae: 0.2545 - learning_rate: 1.2500e-04\n",
            "Epoch 64/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1074 - mae: 0.2585 - val_loss: 0.1083 - val_mae: 0.2513 - learning_rate: 1.2500e-04\n",
            "Epoch 65/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1064 - mae: 0.2529 - val_loss: 0.1088 - val_mae: 0.2526 - learning_rate: 1.2500e-04\n",
            "\n",
            "Evaluating GRU model...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m           GRU OZONE PREDICTION MODEL            \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\u001b[1mModel Information:\u001b[0m\n",
            "  Type: GRU Neural Network\n",
            "  Features: 8\n",
            "  Sequence Length: 1\n",
            "\n",
            "\u001b[1mPerformance Metrics:\u001b[0m\n",
            "  \u001b[1mMAE:\u001b[0m     \u001b[92m13.27 µg/m³\u001b[0m\n",
            "  \u001b[1mRMSE:\u001b[0m    \u001b[92m16.91 µg/m³\u001b[0m\n",
            "  \u001b[1mR²:\u001b[0m      \u001b[92m0.8401\u001b[0m\n",
            "  \u001b[1mMSE:\u001b[0m     285.92\n",
            "\n",
            "\u001b[1mPrediction Summary:\u001b[0m\n",
            "  Average Predicted: 95.51 µg/m³\n",
            "  Average Actual:    93.24 µg/m³\n",
            "  Difference:        2.27 µg/m³\n",
            "\n",
            "\u001b[1mModel Structure:\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model: \"Model9_GRU\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ gru_1 (GRU)                     │ (None, 1, 64)          │        14,208 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout (Dropout)               │ (None, 1, 64)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ gru_2 (GRU)                     │ (None, 32)             │         9,408 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_1 (Dense)                 │ (None, 16)             │           528 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ output (Dense)                  │ (None, 1)              │            17 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 72,485 (283.15 KB)\n",
            " Trainable params: 24,161 (94.38 KB)\n",
            " Non-trainable params: 0 (0.00 B)\n",
            " Optimizer params: 48,324 (188.77 KB)\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m                      END OF REPORT                    \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\n",
            "Creating visualization plots...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to results/models/Model9_GRU.h5\n",
            "\n",
            "Model Comparison:\n",
            "LSTM: MAE = 13.51 µg/m³, RMSE = 17.12 µg/m³, R² = 0.8361\n",
            "GRU: MAE = 13.27 µg/m³, RMSE = 16.91 µg/m³, R² = 0.8401\n",
            "\n",
            "Best model: GRU with MAE = 13.27 µg/m³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convolutional Neural Network model for ozone concentration prediction.\n",
        "Based on Model 11/12 from the research paper.\n",
        "\"\"\"\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "class OzoneCNNModel:\n",
        "    def __init__(self, n_features: int = 12, window_size: int = 7,\n",
        "                 filters: int = 64, kernel_size: int = 3, name: str = \"Model11_CNN\"):\n",
        "        \"\"\"\n",
        "        Initialize the CNN model for ozone prediction.\n",
        "\n",
        "        Args:\n",
        "            n_features (int): Number of input features\n",
        "            window_size (int): Window size for convolution (days)\n",
        "            filters (int): Number of filters in convolution layer\n",
        "            kernel_size (int): Size of the convolution kernel\n",
        "            name (str): Name of the model\n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.window_size = window_size\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.name = name\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = None\n",
        "\n",
        "        # Initialize scalers\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.target_scaler = StandardScaler()\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics = {}\n",
        "        self.history = None\n",
        "\n",
        "        # Feature names (will be set during training)\n",
        "        self.feature_names = []\n",
        "\n",
        "    def _build_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Build the CNN model architecture.\n",
        "        \"\"\"\n",
        "        # Clear previous session to avoid conflicts\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        # Define input shape based on window size and number of features\n",
        "        input_shape = (self.window_size, self.n_features)\n",
        "\n",
        "        # Create sequential model\n",
        "        model = Sequential(name=self.name)\n",
        "\n",
        "        # Add convolutional layers\n",
        "        model.add(Conv1D(filters=self.filters, kernel_size=self.kernel_size,\n",
        "                        activation='relu', padding='same',\n",
        "                        input_shape=input_shape, name='conv_1'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2, name='pool_1'))\n",
        "\n",
        "        model.add(Conv1D(filters=self.filters//2, kernel_size=self.kernel_size,\n",
        "                        activation='relu', padding='same', name='conv_2'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling1D(pool_size=2, name='pool_2'))\n",
        "\n",
        "        # Flatten and add dense layers\n",
        "        model.add(Flatten(name='flatten'))\n",
        "        model.add(Dense(64, activation='relu', name='dense_1'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(32, activation='relu', name='dense_2'))\n",
        "        model.add(Dense(1, activation='linear', name='output'))\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='mse',\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        # Store model\n",
        "        self.model = model\n",
        "\n",
        "    def _prepare_windows(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Prepare input windows for CNN.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Prepared windows\n",
        "        \"\"\"\n",
        "        # Create sliding windows\n",
        "        windows = []\n",
        "        for i in range(len(X) - self.window_size + 1):\n",
        "            windows.append(X[i:i + self.window_size])\n",
        "\n",
        "        return np.array(windows)\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: np.ndarray,\n",
        "              validation_split: float = 0.2, epochs: int = 100,\n",
        "              batch_size: int = 32, patience: int = 20) -> None:\n",
        "        \"\"\"\n",
        "        Train the CNN model.\n",
        "\n",
        "        Args:\n",
        "            X_train (pd.DataFrame): Training features\n",
        "            y_train (np.ndarray): Training targets (ozone concentration)\n",
        "            validation_split (float): Fraction of training data to use for validation\n",
        "            epochs (int): Maximum number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            patience (int): Patience for early stopping\n",
        "        \"\"\"\n",
        "        # Store feature names\n",
        "        self.feature_names = X_train.columns.tolist()\n",
        "        self.n_features = len(self.feature_names)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.feature_scaler.fit_transform(X_train)\n",
        "\n",
        "        # Reshape y_train for scaler\n",
        "        y_reshaped = y_train.reshape(-1, 1)\n",
        "        y_scaled = self.target_scaler.fit_transform(y_reshaped).flatten()\n",
        "\n",
        "        # Prepare input windows\n",
        "        X_windows = self._prepare_windows(X_scaled)\n",
        "\n",
        "        # For windowed data, we need to select corresponding targets\n",
        "        y_windowed = y_scaled[self.window_size-1:]\n",
        "\n",
        "        # Build the model\n",
        "        self._build_model()\n",
        "\n",
        "        # Print model summary\n",
        "        self.model.summary()\n",
        "\n",
        "        # Set up callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_windows, y_windowed,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted ozone concentrations\n",
        "        \"\"\"\n",
        "        # Check if model is built\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        # Check feature order\n",
        "        missing_features = [feat for feat in self.feature_names if feat not in X.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing features: {missing_features}\")\n",
        "\n",
        "        # Ensure correct feature order\n",
        "        X = X[self.feature_names]\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.feature_scaler.transform(X)\n",
        "\n",
        "        # Prepare windows\n",
        "        X_windows = self._prepare_windows(X_scaled)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred_scaled = self.model.predict(X_windows)\n",
        "\n",
        "        # Inverse transform to get original scale\n",
        "        y_pred = self.target_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "        # The predictions are for the last day of each window\n",
        "        # We need to align with the original data, which means we're predicting\n",
        "        # for indices starting at window_size-1\n",
        "        # We'll pad the beginning with NaNs\n",
        "        padded_predictions = np.full(len(X), np.nan)\n",
        "        padded_predictions[self.window_size-1:] = y_pred\n",
        "\n",
        "        return padded_predictions\n",
        "\n",
        "    def evaluate(self, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        # Make predictions\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Get valid indices (non-NaN)\n",
        "        valid_idx = ~np.isnan(y_pred)\n",
        "\n",
        "        # Filter predictions and actual values\n",
        "        y_pred_valid = y_pred[valid_idx]\n",
        "        y_test_valid = y_test[valid_idx]\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "        mse = mean_squared_error(y_test_valid, y_pred_valid)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test_valid, y_pred_valid)\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics = {\n",
        "            'mae': float(mae),\n",
        "            'mse': float(mse),\n",
        "            'rmse': float(rmse),\n",
        "            'r2': float(r2),\n",
        "            'avg_predicted': float(np.mean(y_pred_valid)),\n",
        "            'avg_actual': float(np.mean(y_test_valid)),\n",
        "            'window_size': self.window_size,\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'n_features': self.n_features\n",
        "        }\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_results(self, X_test: pd.DataFrame, y_test: np.ndarray,\n",
        "                    dates_test: pd.DatetimeIndex = None,\n",
        "                    save_dir: str = \"results/figures/cnn\") -> None:\n",
        "        \"\"\"\n",
        "        Create and save plots to visualize model performance.\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): Test features\n",
        "            y_test (np.ndarray): True test targets\n",
        "            dates_test (pd.DatetimeIndex): Test dates for time series plot\n",
        "            save_dir (str): Directory to save the plots\n",
        "        \"\"\"\n",
        "        # Create save directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # Get valid indices (non-NaN)\n",
        "        valid_idx = ~np.isnan(y_pred)\n",
        "\n",
        "        # Filter predictions and actual values\n",
        "        y_pred_valid = y_pred[valid_idx]\n",
        "        y_test_valid = y_test[valid_idx]\n",
        "\n",
        "        # Filter dates if provided\n",
        "        if dates_test is not None:\n",
        "            dates_valid = dates_test[valid_idx]\n",
        "\n",
        "        # 1. Training history plot\n",
        "        if self.history is not None:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            # Plot training & validation loss\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "            plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss During Training')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "\n",
        "            # Plot training & validation MAE\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(self.history.history['mae'], label='Training MAE')\n",
        "            plt.plot(self.history.history['val_mae'], label='Validation MAE')\n",
        "            plt.title('Model MAE During Training')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"training_history.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 2. Actual vs Predicted scatter plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test_valid, y_pred_valid, alpha=0.5)\n",
        "\n",
        "        # Add perfect prediction line\n",
        "        max_val = max(np.max(y_test_valid), np.max(y_pred_valid))\n",
        "        min_val = min(np.min(y_test_valid), np.min(y_pred_valid))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "        plt.xlabel('Actual Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.title('CNN Model: Actual vs Predicted Ozone')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add metrics text\n",
        "        r2 = self.metrics.get('r2', 0)\n",
        "        rmse = self.metrics.get('rmse', 0)\n",
        "        mae = self.metrics.get('mae', 0)\n",
        "\n",
        "        metrics_text = f\"R² = {r2:.3f}\\nRMSE = {rmse:.2f} µg/m³\\nMAE = {mae:.2f} µg/m³\"\n",
        "        plt.annotate(metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"actual_vs_predicted.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Residual plot\n",
        "        residuals = y_test_valid - y_pred_valid\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_pred_valid, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Ozone Concentration (µg/m³)')\n",
        "        plt.ylabel('Residuals (µg/m³)')\n",
        "        plt.title('Residual Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add residual statistics\n",
        "        res_mean = np.mean(residuals)\n",
        "        res_std = np.std(residuals)\n",
        "        res_text = f\"Mean = {res_mean:.2f} µg/m³\\nStd Dev = {res_std:.2f} µg/m³\"\n",
        "        plt.annotate(res_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                     verticalalignment='top',\n",
        "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_plot.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Time series plot (if dates are provided)\n",
        "        if dates_test is not None:\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_valid, y_test_valid, label='Actual', alpha=0.7)\n",
        "            plt.plot(dates_valid, y_pred_valid, label='Predicted', alpha=0.7)\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Ozone Concentration (µg/m³)')\n",
        "            plt.title('Ozone Concentration Over Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 5. Residuals over time\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(dates_valid, residuals, color='royalblue', alpha=0.7)\n",
        "            plt.axhline(y=0, color='r', linestyle='--')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Residuals (µg/m³)')\n",
        "            plt.title('Prediction Residuals Over Time')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, \"residuals_time_series.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # 6. Histogram of residuals\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.xlabel('Residuals (µg/m³)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of Residuals')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_dir, \"residual_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def save_metrics(self, save_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            save_path (str): Path to save the metrics\n",
        "        \"\"\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "    def save_model(self, save_dir: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained model.\n",
        "\n",
        "        Args:\n",
        "            save_dir (str): Directory to save the model\n",
        "        \"\"\"\n",
        "        # Create directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Create model name\n",
        "        model_path = os.path.join(save_dir, f\"{self.name}.h5\")\n",
        "\n",
        "        # Save model\n",
        "        self.model.save(model_path)\n",
        "\n",
        "        # Save scalers\n",
        "        import pickle\n",
        "        with open(os.path.join(save_dir, f\"{self.name}_feature_scaler.pkl\"), 'wb') as f:\n",
        "            pickle.dump(self.feature_scaler, f)\n",
        "\n",
        "        with open(os.path.join(save_dir, f\"{self.name}_target_scaler.pkl\"), 'wb') as f:\n",
        "            pickle.dump(self.target_scaler, f)\n",
        "\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_dir: str, model_name: str) -> 'OzoneCNNModel':\n",
        "        \"\"\"\n",
        "        Load a saved model.\n",
        "\n",
        "        Args:\n",
        "            model_dir (str): Directory where the model is saved\n",
        "            model_name (str): Name of the model\n",
        "\n",
        "        Returns:\n",
        "            OzoneCNNModel: Loaded model\n",
        "        \"\"\"\n",
        "        # Create model path\n",
        "        model_path = os.path.join(model_dir, f\"{model_name}.h5\")\n",
        "\n",
        "        # Load model\n",
        "        loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "        # Load scalers\n",
        "        import pickle\n",
        "        with open(os.path.join(model_dir, f\"{model_name}_feature_scaler.pkl\"), 'rb') as f:\n",
        "            feature_scaler = pickle.load(f)\n",
        "\n",
        "        with open(os.path.join(model_dir, f\"{model_name}_target_scaler.pkl\"), 'rb') as f:\n",
        "            target_scaler = pickle.load(f)\n",
        "\n",
        "        # Create new instance\n",
        "        instance = cls(name=model_name)\n",
        "        instance.model = loaded_model\n",
        "        instance.feature_scaler = feature_scaler\n",
        "        instance.target_scaler = target_scaler\n",
        "\n",
        "        # Try to extract window size and filters from model configuration\n",
        "        for layer in loaded_model.layers:\n",
        "            if isinstance(layer, tf.keras.layers.Conv1D):\n",
        "                instance.filters = layer.filters\n",
        "                instance.kernel_size = layer.kernel_size[0]\n",
        "                instance.window_size = layer.input_shape[1]\n",
        "                break\n",
        "\n",
        "        return instance\n",
        "\n",
        "def display_pretty_results(model, metrics, display_components=True):\n",
        "    \"\"\"\n",
        "    Display model results in a nicely formatted way.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        metrics: Dictionary containing model evaluation metrics\n",
        "        display_components: Whether to display component coefficients\n",
        "    \"\"\"\n",
        "    # Terminal formatting codes\n",
        "    BOLD = '\\033[1m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "\n",
        "    # Print title\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}           CNN OZONE PREDICTION MODEL                 {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "    # Print model information\n",
        "    print(f\"{BOLD}Model Information:{ENDC}\")\n",
        "    print(f\"  Type: Convolutional Neural Network\")\n",
        "    print(f\"  Features: {metrics.get('n_features', 'Unknown')}\")\n",
        "\n",
        "    if 'window_size' in metrics:\n",
        "        print(f\"  Window Size: {metrics['window_size']}\")\n",
        "    if 'filters' in metrics:\n",
        "        print(f\"  Filters: {metrics['filters']}\")\n",
        "    if 'kernel_size' in metrics:\n",
        "        print(f\"  Kernel Size: {metrics['kernel_size']}\")\n",
        "\n",
        "    # Print key metrics\n",
        "    print(f\"\\n{BOLD}Performance Metrics:{ENDC}\")\n",
        "    print(f\"  {BOLD}MAE:{ENDC}     {GREEN}{metrics['mae']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}RMSE:{ENDC}    {GREEN}{metrics['rmse']:.2f} µg/m³{ENDC}\")\n",
        "    print(f\"  {BOLD}R²:{ENDC}      {GREEN}{metrics['r2']:.4f}{ENDC}\")\n",
        "    print(f\"  {BOLD}MSE:{ENDC}     {metrics['mse']:.2f}\")\n",
        "\n",
        "    # Print prediction averages\n",
        "    print(f\"\\n{BOLD}Prediction Summary:{ENDC}\")\n",
        "    print(f\"  Average Predicted: {metrics['avg_predicted']:.2f} µg/m³\")\n",
        "    print(f\"  Average Actual:    {metrics['avg_actual']:.2f} µg/m³\")\n",
        "    print(f\"  Difference:        {abs(metrics['avg_predicted'] - metrics['avg_actual']):.2f} µg/m³\")\n",
        "\n",
        "    # Print model structure if available\n",
        "    if hasattr(model, 'model') and model.model is not None:\n",
        "        print(f\"\\n{BOLD}Model Structure:{ENDC}\")\n",
        "        model.model.summary(print_fn=lambda x: print(f\"  {x}\"))\n",
        "\n",
        "    # Print footer\n",
        "    print(f\"\\n{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}                      END OF REPORT                    {ENDC}\")\n",
        "    print(f\"{BOLD}{BLUE}═══════════════════════════════════════════════════════{ENDC}\\n\")\n",
        "\n",
        "\n",
        "def cnn_ozone_model():\n",
        "    \"\"\"Main function to train and evaluate the CNN model.\"\"\"\n",
        "    # Create results directory\n",
        "    results_dir = \"results\"\n",
        "    model_dir = os.path.join(results_dir, \"models\")\n",
        "    figures_dir = os.path.join(results_dir, \"figures/cnn\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    # Load processed data with all required features\n",
        "    processed_dir = \"data/processed\"\n",
        "    train_data = pd.read_csv(os.path.join(processed_dir, \"train_data_full.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(processed_dir, \"test_data_full.csv\"))\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # Extract features and targets\n",
        "    feature_cols = ['NO2', 'NOX', 'SO2', 'NMVOC', 'TEMP', 'RAD']\n",
        "    # Add derived features like ones in the paper\n",
        "    train_data['DayOfYear_Sin'] = np.sin(2 * np.pi * train_data['date'].dt.dayofyear / 365)\n",
        "    train_data['DayOfYear_Cos'] = np.cos(2 * np.pi * train_data['date'].dt.dayofyear / 365)\n",
        "\n",
        "    test_data['DayOfYear_Sin'] = np.sin(2 * np.pi * test_data['date'].dt.dayofyear / 365)\n",
        "    test_data['DayOfYear_Cos'] = np.cos(2 * np.pi * test_data['date'].dt.dayofyear / 365)\n",
        "\n",
        "    # Extended feature list with seasonality\n",
        "    feature_cols_extended = feature_cols + ['DayOfYear_Sin', 'DayOfYear_Cos']\n",
        "\n",
        "    X_train = train_data[feature_cols_extended]\n",
        "    y_train = train_data['ozone'].values\n",
        "    X_test = test_data[feature_cols_extended]\n",
        "    y_test = test_data['ozone'].values\n",
        "\n",
        "    # Print dataset information\n",
        "    print(\"\\nDataset Information:\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Testing samples: {len(X_test)}\")\n",
        "    print(f\"Features: {', '.join(feature_cols_extended)}\")\n",
        "\n",
        "    # Train CNN model (for Model 11 from the paper)\n",
        "    print(\"\\nTraining CNN model...\")\n",
        "    cnn_model = OzoneCNNModel(n_features=len(feature_cols_extended),\n",
        "                             window_size=7, filters=64, kernel_size=3,\n",
        "                             name=\"Model11_CNN\")\n",
        "\n",
        "    cnn_model.train(X_train, y_train, validation_split=0.2,\n",
        "                   epochs=150, batch_size=32, patience=30)\n",
        "\n",
        "    # Evaluate CNN model\n",
        "    print(\"\\nEvaluating CNN model...\")\n",
        "    cnn_metrics = cnn_model.evaluate(X_test, y_test)\n",
        "\n",
        "    # Display pretty results\n",
        "    display_pretty_results(cnn_model, cnn_metrics)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualization plots...\")\n",
        "    cnn_model.plot_results(X_test, y_test, test_data['date'],\n",
        "                          save_dir=os.path.join(figures_dir, \"cnn\"))\n",
        "\n",
        "    # Save model and metrics\n",
        "    cnn_model.save_metrics(os.path.join(model_dir, \"cnn_metrics.json\"))\n",
        "    cnn_model.save_model(model_dir)\n",
        "\n",
        "    # Try a different window size (for Model 12 from the paper)\n",
        "    print(\"\\nTraining CNN model with larger window...\")\n",
        "    cnn_model2 = OzoneCNNModel(n_features=len(feature_cols_extended),\n",
        "                              window_size=14, filters=64, kernel_size=3,\n",
        "                              name=\"Model12_CNN\")\n",
        "\n",
        "    cnn_model2.train(X_train, y_train, validation_split=0.2,\n",
        "                    epochs=150, batch_size=32, patience=30)\n",
        "\n",
        "    # Evaluate second CNN model\n",
        "    print(\"\\nEvaluating second CNN model...\")\n",
        "    cnn_metrics2 = cnn_model2.evaluate(X_test, y_test)\n",
        "\n",
        "    # Display pretty results\n",
        "    display_pretty_results(cnn_model2, cnn_metrics2)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualization plots...\")\n",
        "    cnn_model2.plot_results(X_test, y_test, test_data['date'],\n",
        "                           save_dir=os.path.join(figures_dir, \"cnn2\"))\n",
        "\n",
        "    # Save model and metrics\n",
        "    cnn_model2.save_metrics(os.path.join(model_dir, \"cnn2_metrics.json\"))\n",
        "    cnn_model2.save_model(model_dir)\n",
        "\n",
        "    # Compare models\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(f\"CNN (Window={cnn_model.window_size}): MAE = {cnn_metrics['mae']:.2f} µg/m³, RMSE = {cnn_metrics['rmse']:.2f} µg/m³, R² = {cnn_metrics['r2']:.4f}\")\n",
        "    print(f\"CNN (Window={cnn_model2.window_size}): MAE = {cnn_metrics2['mae']:.2f} µg/m³, RMSE = {cnn_metrics2['rmse']:.2f} µg/m³, R² = {cnn_metrics2['r2']:.4f}\")\n",
        "\n",
        "    # Return the best model\n",
        "    if cnn_metrics['mae'] <= cnn_metrics2['mae']:\n",
        "        print(f\"\\nBest model: CNN (Window={cnn_model.window_size}) with MAE = {cnn_metrics['mae']:.2f} µg/m³\")\n",
        "        return cnn_model, cnn_metrics\n",
        "    else:\n",
        "        print(f\"\\nBest model: CNN (Window={cnn_model2.window_size}) with MAE = {cnn_metrics2['mae']:.2f} µg/m³\")\n",
        "        return cnn_model2, cnn_metrics2\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cnn_ozone_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QhQzQq_QNbzo",
        "outputId": "139b7ff3-5672-4736-d54c-ab576d85157d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Information:\n",
            "Training samples: 2337\n",
            "Testing samples: 585\n",
            "Features: NO2, NOX, SO2, NMVOC, TEMP, RAD, DayOfYear_Sin, DayOfYear_Cos\n",
            "\n",
            "Training CNN model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Model11_CNN\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model11_CNN\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m1,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,385\u001b[0m (48.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,385</span> (48.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,193\u001b[0m (47.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,193</span> (47.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6888 - mae: 0.6470 - val_loss: 0.4492 - val_mae: 0.5175 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2956 - mae: 0.4359 - val_loss: 0.3588 - val_mae: 0.4674 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2349 - mae: 0.3888 - val_loss: 0.2778 - val_mae: 0.4124 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2624 - mae: 0.4050 - val_loss: 0.2651 - val_mae: 0.4007 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2130 - mae: 0.3631 - val_loss: 0.2271 - val_mae: 0.3767 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1931 - mae: 0.3519 - val_loss: 0.1948 - val_mae: 0.3468 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1940 - mae: 0.3507 - val_loss: 0.1581 - val_mae: 0.3138 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1683 - mae: 0.3245 - val_loss: 0.1528 - val_mae: 0.3106 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1686 - mae: 0.3197 - val_loss: 0.1920 - val_mae: 0.3370 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1559 - mae: 0.3103 - val_loss: 0.1454 - val_mae: 0.3035 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1474 - mae: 0.3033 - val_loss: 0.1710 - val_mae: 0.3175 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1385 - mae: 0.2912 - val_loss: 0.1965 - val_mae: 0.3436 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1373 - mae: 0.2878 - val_loss: 0.1548 - val_mae: 0.3114 - learning_rate: 0.0010\n",
            "Epoch 14/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1290 - mae: 0.2852 - val_loss: 0.1563 - val_mae: 0.3173 - learning_rate: 0.0010\n",
            "Epoch 15/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1230 - mae: 0.2740 - val_loss: 0.1529 - val_mae: 0.3099 - learning_rate: 0.0010\n",
            "Epoch 16/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1280 - mae: 0.2802 - val_loss: 0.1391 - val_mae: 0.2959 - learning_rate: 0.0010\n",
            "Epoch 17/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1146 - mae: 0.2652 - val_loss: 0.1649 - val_mae: 0.3111 - learning_rate: 0.0010\n",
            "Epoch 18/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1330 - mae: 0.2855 - val_loss: 0.1428 - val_mae: 0.2991 - learning_rate: 0.0010\n",
            "Epoch 19/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1204 - mae: 0.2703 - val_loss: 0.1623 - val_mae: 0.3112 - learning_rate: 0.0010\n",
            "Epoch 20/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1068 - mae: 0.2506 - val_loss: 0.1408 - val_mae: 0.3034 - learning_rate: 0.0010\n",
            "Epoch 21/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1088 - mae: 0.2561 - val_loss: 0.1495 - val_mae: 0.3070 - learning_rate: 0.0010\n",
            "Epoch 22/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1097 - mae: 0.2600 - val_loss: 0.1704 - val_mae: 0.3200 - learning_rate: 0.0010\n",
            "Epoch 23/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0993 - mae: 0.2396 - val_loss: 0.1485 - val_mae: 0.3058 - learning_rate: 0.0010\n",
            "Epoch 24/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0984 - mae: 0.2451 - val_loss: 0.1489 - val_mae: 0.3065 - learning_rate: 0.0010\n",
            "Epoch 25/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1007 - mae: 0.2464 - val_loss: 0.1454 - val_mae: 0.2997 - learning_rate: 0.0010\n",
            "Epoch 26/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0949 - mae: 0.2409 - val_loss: 0.1405 - val_mae: 0.2991 - learning_rate: 0.0010\n",
            "Epoch 27/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0886 - mae: 0.2313 - val_loss: 0.1434 - val_mae: 0.2972 - learning_rate: 5.0000e-04\n",
            "Epoch 28/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0912 - mae: 0.2382 - val_loss: 0.1484 - val_mae: 0.3052 - learning_rate: 5.0000e-04\n",
            "Epoch 29/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0865 - mae: 0.2315 - val_loss: 0.1509 - val_mae: 0.3115 - learning_rate: 5.0000e-04\n",
            "Epoch 30/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0799 - mae: 0.2184 - val_loss: 0.1420 - val_mae: 0.3008 - learning_rate: 5.0000e-04\n",
            "Epoch 31/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0796 - mae: 0.2235 - val_loss: 0.1399 - val_mae: 0.2982 - learning_rate: 5.0000e-04\n",
            "Epoch 32/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0830 - mae: 0.2236 - val_loss: 0.1532 - val_mae: 0.3074 - learning_rate: 5.0000e-04\n",
            "Epoch 33/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0870 - mae: 0.2263 - val_loss: 0.1495 - val_mae: 0.3066 - learning_rate: 5.0000e-04\n",
            "Epoch 34/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0799 - mae: 0.2128 - val_loss: 0.1750 - val_mae: 0.3230 - learning_rate: 5.0000e-04\n",
            "Epoch 35/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0735 - mae: 0.2090 - val_loss: 0.1563 - val_mae: 0.3141 - learning_rate: 5.0000e-04\n",
            "Epoch 36/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0813 - mae: 0.2241 - val_loss: 0.1648 - val_mae: 0.3209 - learning_rate: 5.0000e-04\n",
            "Epoch 37/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0763 - mae: 0.2155 - val_loss: 0.1731 - val_mae: 0.3235 - learning_rate: 2.5000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0729 - mae: 0.2091 - val_loss: 0.1643 - val_mae: 0.3173 - learning_rate: 2.5000e-04\n",
            "Epoch 39/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0712 - mae: 0.2045 - val_loss: 0.1598 - val_mae: 0.3157 - learning_rate: 2.5000e-04\n",
            "Epoch 40/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0761 - mae: 0.2127 - val_loss: 0.1548 - val_mae: 0.3115 - learning_rate: 2.5000e-04\n",
            "Epoch 41/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0722 - mae: 0.2110 - val_loss: 0.1572 - val_mae: 0.3149 - learning_rate: 2.5000e-04\n",
            "Epoch 42/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0731 - mae: 0.2136 - val_loss: 0.1485 - val_mae: 0.3093 - learning_rate: 2.5000e-04\n",
            "Epoch 43/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0764 - mae: 0.2102 - val_loss: 0.1606 - val_mae: 0.3148 - learning_rate: 2.5000e-04\n",
            "Epoch 44/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0714 - mae: 0.2069 - val_loss: 0.1607 - val_mae: 0.3178 - learning_rate: 2.5000e-04\n",
            "Epoch 45/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0717 - mae: 0.2090 - val_loss: 0.1663 - val_mae: 0.3206 - learning_rate: 2.5000e-04\n",
            "Epoch 46/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0659 - mae: 0.2003 - val_loss: 0.1489 - val_mae: 0.3083 - learning_rate: 2.5000e-04\n",
            "\n",
            "Evaluating CNN model...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m           CNN OZONE PREDICTION MODEL                 \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\u001b[1mModel Information:\u001b[0m\n",
            "  Type: Convolutional Neural Network\n",
            "  Features: 8\n",
            "  Window Size: 7\n",
            "  Filters: 64\n",
            "  Kernel Size: 3\n",
            "\n",
            "\u001b[1mPerformance Metrics:\u001b[0m\n",
            "  \u001b[1mMAE:\u001b[0m     \u001b[92m14.46 µg/m³\u001b[0m\n",
            "  \u001b[1mRMSE:\u001b[0m    \u001b[92m18.21 µg/m³\u001b[0m\n",
            "  \u001b[1mR²:\u001b[0m      \u001b[92m0.8155\u001b[0m\n",
            "  \u001b[1mMSE:\u001b[0m     331.61\n",
            "\n",
            "\u001b[1mPrediction Summary:\u001b[0m\n",
            "  Average Predicted: 96.37 µg/m³\n",
            "  Average Actual:    93.03 µg/m³\n",
            "  Difference:        3.34 µg/m³\n",
            "\n",
            "\u001b[1mModel Structure:\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model: \"Model11_CNN\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ conv_1 (Conv1D)                 │ (None, 7, 64)          │         1,600 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization             │ (None, 7, 64)          │           256 │\n",
            "│ (BatchNormalization)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pool_1 (MaxPooling1D)           │ (None, 3, 64)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ conv_2 (Conv1D)                 │ (None, 3, 32)          │         6,176 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization_1           │ (None, 3, 32)          │           128 │\n",
            "│ (BatchNormalization)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pool_2 (MaxPooling1D)           │ (None, 1, 32)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ flatten (Flatten)               │ (None, 32)             │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_1 (Dense)                 │ (None, 64)             │         2,112 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout (Dropout)               │ (None, 64)             │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_2 (Dense)                 │ (None, 32)             │         2,080 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ output (Dense)                  │ (None, 1)              │            33 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 36,773 (143.65 KB)\n",
            " Trainable params: 12,193 (47.63 KB)\n",
            " Non-trainable params: 192 (768.00 B)\n",
            " Optimizer params: 24,388 (95.27 KB)\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m                      END OF REPORT                    \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\n",
            "Creating visualization plots...\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to results/models/Model11_CNN.h5\n",
            "\n",
            "Training CNN model with larger window...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Model12_CNN\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model12_CNN\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m1,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │         \u001b[38;5;34m6,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,481\u001b[0m (64.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,481</span> (64.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,289\u001b[0m (63.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,289</span> (63.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.6107 - mae: 0.6273 - val_loss: 0.4910 - val_mae: 0.5376 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3432 - mae: 0.4672 - val_loss: 0.3989 - val_mae: 0.4872 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2788 - mae: 0.4156 - val_loss: 0.3913 - val_mae: 0.4829 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2428 - mae: 0.3896 - val_loss: 0.3682 - val_mae: 0.4629 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2338 - mae: 0.3851 - val_loss: 0.3439 - val_mae: 0.4521 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2011 - mae: 0.3530 - val_loss: 0.2595 - val_mae: 0.4016 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1934 - mae: 0.3500 - val_loss: 0.2699 - val_mae: 0.4118 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1877 - mae: 0.3446 - val_loss: 0.2464 - val_mae: 0.3959 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1766 - mae: 0.3271 - val_loss: 0.2317 - val_mae: 0.3796 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1700 - mae: 0.3287 - val_loss: 0.2419 - val_mae: 0.3853 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1446 - mae: 0.3016 - val_loss: 0.2007 - val_mae: 0.3501 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1371 - mae: 0.2932 - val_loss: 0.2064 - val_mae: 0.3569 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1666 - mae: 0.3198 - val_loss: 0.2540 - val_mae: 0.3868 - learning_rate: 0.0010\n",
            "Epoch 14/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1304 - mae: 0.2868 - val_loss: 0.1744 - val_mae: 0.3239 - learning_rate: 0.0010\n",
            "Epoch 15/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1355 - mae: 0.2911 - val_loss: 0.1954 - val_mae: 0.3396 - learning_rate: 0.0010\n",
            "Epoch 16/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1275 - mae: 0.2827 - val_loss: 0.1827 - val_mae: 0.3357 - learning_rate: 0.0010\n",
            "Epoch 17/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1271 - mae: 0.2820 - val_loss: 0.2184 - val_mae: 0.3585 - learning_rate: 0.0010\n",
            "Epoch 18/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1163 - mae: 0.2653 - val_loss: 0.1890 - val_mae: 0.3395 - learning_rate: 0.0010\n",
            "Epoch 19/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1202 - mae: 0.2671 - val_loss: 0.2338 - val_mae: 0.3738 - learning_rate: 0.0010\n",
            "Epoch 20/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1171 - mae: 0.2658 - val_loss: 0.1920 - val_mae: 0.3424 - learning_rate: 0.0010\n",
            "Epoch 21/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1125 - mae: 0.2643 - val_loss: 0.1879 - val_mae: 0.3419 - learning_rate: 0.0010\n",
            "Epoch 22/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1049 - mae: 0.2527 - val_loss: 0.1479 - val_mae: 0.3057 - learning_rate: 0.0010\n",
            "Epoch 23/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1120 - mae: 0.2590 - val_loss: 0.1621 - val_mae: 0.3169 - learning_rate: 0.0010\n",
            "Epoch 24/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0926 - mae: 0.2353 - val_loss: 0.1761 - val_mae: 0.3327 - learning_rate: 0.0010\n",
            "Epoch 25/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0937 - mae: 0.2391 - val_loss: 0.1607 - val_mae: 0.3164 - learning_rate: 0.0010\n",
            "Epoch 26/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1257 - mae: 0.2746 - val_loss: 0.1381 - val_mae: 0.2877 - learning_rate: 0.0010\n",
            "Epoch 27/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0904 - mae: 0.2331 - val_loss: 0.1619 - val_mae: 0.3131 - learning_rate: 0.0010\n",
            "Epoch 28/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1077 - mae: 0.2554 - val_loss: 0.1947 - val_mae: 0.3389 - learning_rate: 0.0010\n",
            "Epoch 29/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0904 - mae: 0.2320 - val_loss: 0.1538 - val_mae: 0.3064 - learning_rate: 0.0010\n",
            "Epoch 30/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0822 - mae: 0.2229 - val_loss: 0.1442 - val_mae: 0.3009 - learning_rate: 0.0010\n",
            "Epoch 31/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0855 - mae: 0.2222 - val_loss: 0.1674 - val_mae: 0.3195 - learning_rate: 0.0010\n",
            "Epoch 32/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0889 - mae: 0.2325 - val_loss: 0.1941 - val_mae: 0.3440 - learning_rate: 0.0010\n",
            "Epoch 33/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0867 - mae: 0.2261 - val_loss: 0.1598 - val_mae: 0.3145 - learning_rate: 0.0010\n",
            "Epoch 34/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0811 - mae: 0.2148 - val_loss: 0.1678 - val_mae: 0.3231 - learning_rate: 0.0010\n",
            "Epoch 35/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0859 - mae: 0.2282 - val_loss: 0.1527 - val_mae: 0.3094 - learning_rate: 0.0010\n",
            "Epoch 36/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0893 - mae: 0.2338 - val_loss: 0.1614 - val_mae: 0.3154 - learning_rate: 0.0010\n",
            "Epoch 37/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0690 - mae: 0.2078 - val_loss: 0.1594 - val_mae: 0.3119 - learning_rate: 5.0000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0685 - mae: 0.2017 - val_loss: 0.1575 - val_mae: 0.3124 - learning_rate: 5.0000e-04\n",
            "Epoch 39/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0641 - mae: 0.1953 - val_loss: 0.1635 - val_mae: 0.3147 - learning_rate: 5.0000e-04\n",
            "Epoch 40/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0632 - mae: 0.1973 - val_loss: 0.1595 - val_mae: 0.3132 - learning_rate: 5.0000e-04\n",
            "Epoch 41/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0611 - mae: 0.1911 - val_loss: 0.1444 - val_mae: 0.2975 - learning_rate: 5.0000e-04\n",
            "Epoch 42/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0569 - mae: 0.1875 - val_loss: 0.1852 - val_mae: 0.3289 - learning_rate: 5.0000e-04\n",
            "Epoch 43/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0586 - mae: 0.1850 - val_loss: 0.1778 - val_mae: 0.3224 - learning_rate: 5.0000e-04\n",
            "Epoch 44/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0621 - mae: 0.1943 - val_loss: 0.1557 - val_mae: 0.3083 - learning_rate: 5.0000e-04\n",
            "Epoch 45/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0588 - mae: 0.1868 - val_loss: 0.1534 - val_mae: 0.3069 - learning_rate: 5.0000e-04\n",
            "Epoch 46/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0556 - mae: 0.1825 - val_loss: 0.1642 - val_mae: 0.3165 - learning_rate: 5.0000e-04\n",
            "Epoch 47/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0599 - mae: 0.1901 - val_loss: 0.1597 - val_mae: 0.3131 - learning_rate: 2.5000e-04\n",
            "Epoch 48/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0577 - mae: 0.1844 - val_loss: 0.1763 - val_mae: 0.3270 - learning_rate: 2.5000e-04\n",
            "Epoch 49/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0523 - mae: 0.1753 - val_loss: 0.1597 - val_mae: 0.3131 - learning_rate: 2.5000e-04\n",
            "Epoch 50/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0529 - mae: 0.1754 - val_loss: 0.1651 - val_mae: 0.3171 - learning_rate: 2.5000e-04\n",
            "Epoch 51/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0540 - mae: 0.1808 - val_loss: 0.1566 - val_mae: 0.3099 - learning_rate: 2.5000e-04\n",
            "Epoch 52/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0517 - mae: 0.1761 - val_loss: 0.1582 - val_mae: 0.3121 - learning_rate: 2.5000e-04\n",
            "Epoch 53/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0526 - mae: 0.1738 - val_loss: 0.1577 - val_mae: 0.3112 - learning_rate: 2.5000e-04\n",
            "Epoch 54/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0503 - mae: 0.1704 - val_loss: 0.1711 - val_mae: 0.3206 - learning_rate: 2.5000e-04\n",
            "Epoch 55/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0478 - mae: 0.1702 - val_loss: 0.1598 - val_mae: 0.3116 - learning_rate: 2.5000e-04\n",
            "Epoch 56/150\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0498 - mae: 0.1717 - val_loss: 0.1725 - val_mae: 0.3213 - learning_rate: 2.5000e-04\n",
            "\n",
            "Evaluating second CNN model...\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m           CNN OZONE PREDICTION MODEL                 \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\u001b[1mModel Information:\u001b[0m\n",
            "  Type: Convolutional Neural Network\n",
            "  Features: 8\n",
            "  Window Size: 14\n",
            "  Filters: 64\n",
            "  Kernel Size: 3\n",
            "\n",
            "\u001b[1mPerformance Metrics:\u001b[0m\n",
            "  \u001b[1mMAE:\u001b[0m     \u001b[92m14.22 µg/m³\u001b[0m\n",
            "  \u001b[1mRMSE:\u001b[0m    \u001b[92m17.98 µg/m³\u001b[0m\n",
            "  \u001b[1mR²:\u001b[0m      \u001b[92m0.8213\u001b[0m\n",
            "  \u001b[1mMSE:\u001b[0m     323.20\n",
            "\n",
            "\u001b[1mPrediction Summary:\u001b[0m\n",
            "  Average Predicted: 97.18 µg/m³\n",
            "  Average Actual:    92.78 µg/m³\n",
            "  Difference:        4.40 µg/m³\n",
            "\n",
            "\u001b[1mModel Structure:\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model: \"Model12_CNN\"\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ conv_1 (Conv1D)                 │ (None, 14, 64)         │         1,600 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization             │ (None, 14, 64)         │           256 │\n",
            "│ (BatchNormalization)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pool_1 (MaxPooling1D)           │ (None, 7, 64)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ conv_2 (Conv1D)                 │ (None, 7, 32)          │         6,176 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization_1           │ (None, 7, 32)          │           128 │\n",
            "│ (BatchNormalization)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pool_2 (MaxPooling1D)           │ (None, 3, 32)          │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ flatten (Flatten)               │ (None, 96)             │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_1 (Dense)                 │ (None, 64)             │         6,208 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dropout (Dropout)               │ (None, 64)             │             0 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense_2 (Dense)                 │ (None, 32)             │         2,080 │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ output (Dense)                  │ (None, 1)              │            33 │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            " Total params: 49,061 (191.65 KB)\n",
            " Trainable params: 16,289 (63.63 KB)\n",
            " Non-trainable params: 192 (768.00 B)\n",
            " Optimizer params: 32,580 (127.27 KB)\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\u001b[1m\u001b[94m                      END OF REPORT                    \u001b[0m\n",
            "\u001b[1m\u001b[94m═══════════════════════════════════════════════════════\u001b[0m\n",
            "\n",
            "\n",
            "Creating visualization plots...\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to results/models/Model12_CNN.h5\n",
            "\n",
            "Model Comparison:\n",
            "CNN (Window=7): MAE = 14.46 µg/m³, RMSE = 18.21 µg/m³, R² = 0.8155\n",
            "CNN (Window=14): MAE = 14.22 µg/m³, RMSE = 17.98 µg/m³, R² = 0.8213\n",
            "\n",
            "Best model: CNN (Window=14) with MAE = 14.22 µg/m³\n"
          ]
        }
      ]
    }
  ]
}